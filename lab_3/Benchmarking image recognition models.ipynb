{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking image recognition models\n",
    "\n",
    "The `LeNet` architecture is an example of a convolutional network architecture that emerged in the 1990.\n",
    "Since then, the architecture of the networks have been improved and their complexity has increased.\n",
    "The complexity of machine learning models can be quantified with the notion of [neuronal capacity](https://proceedings.neurips.cc/paper/2018/file/a292f1c5874b2be8395ffd75f313937f-Paper.pdf), which is related to the number of trainable parameters and the number of layers in the network.\n",
    "Below you see a bar chart showing the evolution of the error rate as the models become more complex.\n",
    "![image](./imagenet-learnopencv_com-generative-and-discriminative-models.png)\n",
    "\n",
    "When addressing a problem with machine learning methods, a large part of the work is to compare models and their performances on some datasets of interests.\n",
    "\n",
    "\n",
    "We propose to benchmark the performances of models with different architectures and complexity on image classification datasets. In order to save some training time, the models can be downloaded pre-trained on a large image dataset: [ImageNet](https://pytorch.org/vision/master/generated/torchvision.datasets.ImageNet.html) dataset. \n",
    "We will then re-train the models to fit our new image datasets. This touches upon the field of transfer learning which you will study in more details later in the course. You can also download models without pre-training.\n",
    "\n",
    "\n",
    "Depending on your interest you might also want to define performance and complexity in different ways: \n",
    "- Performances can be understood as the test performances (e.g. accuracy), but for critical applications the test runtime can also be an important factor.\n",
    "- Complexity can be understood for instance as the number of trainable parameters, the number of layers, the amount of memory required for training or the training time.\n",
    "\n",
    "\n",
    "## Task\n",
    "You are then free to attempt to answer different questions depending on your interests.\n",
    "For instance:\n",
    "1. What model gives the best performance/complexity tradeoff for a particular dataset ?\n",
    "2. What model gives the best performance accross datasets ?\n",
    "\n",
    "You could first choose at least two models and two datasets. Then train/retrain the models on each dataset and average the results accross datasets to get one \"score\" per model.\n",
    "\n",
    "3. What model requires the most training/retraining to achieve a certain performance on a new dataset ?\n",
    "\n",
    "Since the models and datasets might have mis-matching input and output sizes, a first task will be to make sure that the dimension of the networks and the dimension of the datasets match.\n",
    "The models can then be re-trained on a new datasets and their performances evaluated.\n",
    "\n",
    "\n",
    "We suggest to organize the work as follows:\n",
    "- Choose a question to answer (among the list above or one of your own)\n",
    "- Choose at least one dataset and two models or one model and two datasets to compare\n",
    "- Report the test accuracy of the models before retraining. This is to ensure that the model input and output sizes match the datasets you are using\n",
    "- Train the models on the new datasets\n",
    "- Propose an answer to your question\n",
    "\n",
    "\n",
    "\n",
    "##  References\n",
    "- The [ImageNet](https://pytorch.org/vision/master/generated/torchvision.datasets.ImageNet.html) dataset contains around 1.2 million images to be classified into 1000 classes.\n",
    "- The models are taken from the Pytorch vision model database: https://pytorch.org/hub/research-models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch==1.10.1+cu102\n",
      "torchvision==0.11.2+cu102\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "print(\"torch=={}\".format(torch.__version__))\n",
    "print(\"torchvision=={}\".format(torchvision.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now declare a dictionary containing the model instantiation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      " - resnet18\n",
      " - alexnet\n",
      " - squeezenet\n",
      " - vgg16\n",
      " - densenet\n",
      " - inception\n",
      " - googlenet\n",
      " - shufflenet\n",
      " - mobilenet\n",
      " - resnext50_32x4d\n",
      " - wide_resnet50_2\n",
      " - mnasnet1_0\n"
     ]
    }
   ],
   "source": [
    "m = {\"resnet18\":lambda :models.resnet18(pretrained=True),\n",
    "     \"alexnet\":lambda :models.alexnet(pretrained=True),\n",
    "     \"squeezenet\":lambda:models.squeezenet1_1(pretrained=True),\n",
    "     \"vgg16\":lambda :models.vgg16(pretrained=True),\n",
    "     \"densenet\":lambda :models.densenet161(pretrained=True),\n",
    "     \"inception\":lambda :models.inception_v3(pretrained=True),\n",
    "     \"googlenet\":lambda :models.googlenet(pretrained=True),\n",
    "     \"shufflenet\": lambda :models.shufflenet_v2_x1_0(pretrained=True),\n",
    "     \"mobilenet\": lambda :models.mobilenet_v2(pretrained=True),\n",
    "     \"resnext50_32x4d\": lambda :models.resnext50_32x4d(pretrained=True),\n",
    "     \"wide_resnet50_2\":lambda :models.wide_resnet50_2(pretrained=True),\n",
    "     \"mnasnet1_0\":lambda :models.mnasnet1_0(pretrained=True)\n",
    "    }\n",
    "\n",
    "print(\"Available models:\")\n",
    "print(\"\\n\".join([\" - \" + k for k in m.keys()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, to declare an instance of the model called `resnet18`, run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = m[\"resnet18\"]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU\n",
    "\n",
    "The `torch.cuda` API implements functions to interact with GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device=\"cuda\" # You can also use a specific device, e.g. \"cuda:0\", \"cuda:1\" depending on your install\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To push the parameters of the model to a specific `device`, use the `.to()` method with the device you want as argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access the parameter tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can check the parameter tensor iterable:\n",
    "- If the attribute `requires_grad` is `True` then the parameter is trainable\n",
    "- The attribute `device` refers to device the tensor is declared on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requires_grad=False, device=cpu\n",
      "requires_grad=False, device=cpu\n",
      "requires_grad=False, device=cpu\n",
      "requires_grad=False, device=cpu\n",
      "requires_grad=False, device=cpu\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "params_info=[\"requires_grad={}, device={}\".format(p.requires_grad, p.device) for p in net.parameters()]\n",
    "print(\"\\n\".join(params_info[:5]))\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a nicer display of the model layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this display you can see that the layers are defined using instances of the `nn.Sequential` class.\n",
    "This allows you to declare a sequence of layers without having to write the `forward` pass for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual layers are direct attributes of the model object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.layer4[0].bn2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets:\n",
      " - CIFAR10\n",
      " - CIFAR100\n",
      " - MNIST\n",
      " - FashionMNIST\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()]\n",
    ")\n",
    "\n",
    "kwargs = {\"transform\":transform, \"download\":True, \"root\":\"../data\"}\n",
    "\n",
    "\n",
    "d = {\"CIFAR10\": lambda:(datasets.CIFAR10(train=True,**kwargs),datasets.CIFAR10(train=False,**kwargs)),\n",
    "     \"CIFAR100\": lambda:(datasets.CIFAR100(train=True,**kwargs),datasets.CIFAR100(train=False,**kwargs)),\n",
    "     \"MNIST\": lambda:(datasets.MNIST(train=True,**kwargs),datasets.MNIST(train=False,**kwargs)),\n",
    "     \"FashionMNIST\":lambda:(datasets.FashionMNIST(train=True,**kwargs),datasets.FashionMNIST(train=False,**kwargs))\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Available datasets:\")\n",
    "print(\"\\n\".join([\" - \" + k for k in d.keys()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, to get the training and testing sets for the `CIFAR100` dataset, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"CIFAR100\"\n",
    "trainset,testset = d[dataset_name]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare the dimensions of our input dataset with the dimensions of the input and output layers of our network. Depending on the dataset/model pair that you have chosen, you might need to modify the input and output layers of the network to be able to use it on your dataset. You could also find a way to resize the input images using the `torchvision.transforms` module.\n",
    "\n",
    "Note: `net.conv1` and `net.fc` are specific to the `resnet18` network, the attribute names might have to be adapted if you use a different network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image shape: (32, 32, 3)\n",
      "Network input layer ... Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "\n",
      "Number of classes: 100\n",
      "Network output layer ... Linear(in_features=512, out_features=1000, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"Input image shape:\", trainset.data[0].shape)\n",
    "print(\"Network input layer ...\", net.conv1)\n",
    "print()\n",
    "print(\"Number of classes:\", len(trainset.classes))\n",
    "print(\"Network output layer ...\", net.fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data loaders may be obtained as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a test function which prints the accuracy of a model for a given dataset:\n",
    "# Make sure that the data and the model are on the same device.\n",
    "\n",
    "def run_test(model, dataloader):\n",
    "    \"\"\"\n",
    "    Given\n",
    "        model: model class assumed to have a forward method\n",
    "        dataloader: Input data loader\n",
    "    \n",
    "    Prints/Returns\n",
    "        model accuracy on the dataset\n",
    "    \"\"\"\n",
    "    net.eval() # Put the model in eval mode, i.e. disable dropout layers and put the batch norm layers in eval mode\n",
    "\n",
    "    # Write code here:\n",
    "    # ...\n",
    "    # ...     \n",
    "    print(\"Not yet implemented.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the test function is declared, you can run it on the downloaded model and on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not yet implemented.\n"
     ]
    }
   ],
   "source": [
    "run_test(net, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now write the training function.\n",
    "Make sure that the data and the model are on the same device.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(net, dataloader):\n",
    "    \n",
    "    # Choose the number of epoch\n",
    "    n_epoch = 2\n",
    "\n",
    "    # Choose a criterion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Put the model in training mode (i.e. activate batch norm and dropout layers)\n",
    "    net.train()\n",
    "\n",
    "    # Choose an optimizer\n",
    "    optimizer = torch.optim.Adam(net.parameters(),lr=1e-3)\n",
    "\n",
    "\n",
    "    # Implement a training algorithm for your model\n",
    "    for epoch in range(n_epoch):\n",
    "        running_loss=0.\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # Write code here:\n",
    "            # ...\n",
    "            # ... \n",
    "            pass\n",
    "    print(\"Not yet implemented.\")\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this is done you may run the training algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not yet implemented.\n"
     ]
    }
   ],
   "source": [
    "net=run_train(net, trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run the test once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not yet implemented.\n"
     ]
    }
   ],
   "source": [
    "run_test(net, testloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning2022",
   "language": "python",
   "name": "deeplearning2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
