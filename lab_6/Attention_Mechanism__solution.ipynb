{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this notebook we will explore the use of attention mechanisms to solve a machine translation task.\n",
    "\n",
    "The goal of this tutorial is (1) to understand the structure of a sequence-to-sequence model and (2) to identify where and how the attention mechanism intervene in the model archicture.\n",
    "This tutorial is also an occasion to practice the use of recurrent neural network in a context that is different from a classification or a regression task.\n",
    "\n",
    "\n",
    "### seq2seq\n",
    "The model we will use is a sequence-to-sequence model (see picture below) which is the state of the art for a machine translation task, i.e. a model that should learn to transform a sequence of word in language A into a sequence of words into another language B. In the litterature these models are also refered to as `Transformers`.\n",
    "\n",
    "The forward pass through a seq2seq model is a two stage process:\n",
    "1. The input sequence is fed to an `Encoder` model. The role of the `encoder` is to learn a representation of the input sequence that is useful to the second stage.\n",
    "2. The representation of the input sequence is fed to a `Decoder` model. The role of the `decoder` is to use the representation learned by the encoder to produce the desired output sequence.\n",
    "\n",
    "The representation is often called a latent representation or a hidden representation. That is because it is only manipulated internally by the Encoder-Decoder model.\n",
    "Because the inputs and outputs are sequences, the latent representation is also a sequence.\n",
    "\n",
    "![](./seq2seq.jpg)\n",
    "\n",
    "\n",
    "## Tasks\n",
    "\n",
    "The tasks for this lab are to:\n",
    "- Finish the implementation of the Bahdanau attention weights computation\n",
    "- Finish the implementation of the training function\n",
    "- Edit the forward function of the decoder to output the attention weights\n",
    "\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We will use English to German sentence pairs obtained from the Tatoeba Project.\n",
    "The compiled sentence pairs can be found [here](https://www.manythings.org/anki/).\n",
    "\n",
    "\n",
    "## Supplementary\n",
    "- To improve the training performance, and potentially the test performance, you can implement a batch data loader for the training dataset and testing dataset.\n",
    "\n",
    "\n",
    "##  References\n",
    "\n",
    "- An introduction to recurrent neural networks is available [here](https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/)\n",
    "- The attention mechanism were first introduced in the article by Dzmitry Bahdanau et al. available on [arXiv](https://arxiv.org/abs/1409.0473).\n",
    "- This tutorial is adapted from this [blog post](https://blog.floydhub.com/attention-mechanism)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the relevant libraries and defining the device we are running our training on (GPU/CPU).\n",
    "It is advised to use a GPU for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:cpu\n",
      "\n",
      "python:\n",
      "3.8.10 (default, Mar 15 2022, 12:22:08) \n",
      "[GCC 9.4.0]\n",
      "\n",
      "torch==1.6.0\n",
      "numpy==1.21.2\n",
      "spacy==3.2.3\n",
      "tqdm==4.63.0\n",
      "matplotlib==3.4.3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import spacy # The language \"tokenizers\"\n",
    "import tqdm as tqdm_ \n",
    "from tqdm.notebook import tqdm # Overkill progress bars\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.de import German\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"device:{}\\n\".format(device))\n",
    "print(\"python:\\n{}\\n\".format(sys.version))\n",
    "print(\"torch=={}\".format(torch.__version__))\n",
    "print(\"numpy=={}\".format(np.__version__))\n",
    "print(\"spacy=={}\".format(spacy.__version__))\n",
    "print(\"tqdm=={}\".format(tqdm_.__version__))\n",
    "print(\"matplotlib=={}\".format(matplotlib.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "In the next code block, weâ€™ll be doing our data preprocessing steps:\n",
    "\n",
    " - Tokenizing the sentences and creating our vocabulary dictionaries\n",
    " - Assigning each word in our vocabulary to integer indexes\n",
    " - Converting our sentences into their word token indexes\n",
    "\n",
    "There is nothing to do in this part. You can see the results of this step in the cell bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcbe5385175a48cfbb26253176632a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:\n",
      "x: Where's my tip? -> [141, 10, 32, 1390, 6, 1] | y: Wo ist mein Trinkgeld? -> [139, 6, 58, 1903, 8, 1]\n",
      "x: Where's my tip? -> [141, 10, 32, 1390, 6, 1] | y: Wo bleibt mein Trinkgeld? -> [139, 324, 58, 1903, 8, 1]\n"
     ]
    }
   ],
   "source": [
    "# Reading the English-German sentences pairs from the file\n",
    "with open(\"deu.txt\",\"r+\") as file:\n",
    "    deu = [x[:-1] for x in file.readlines()]\n",
    "en = []\n",
    "de = []\n",
    "for line in deu:\n",
    "    en.append(line.split(\"\\t\")[0])\n",
    "    de.append(line.split(\"\\t\")[1])\n",
    "\n",
    "# Setting the number of training sentences we'll use\n",
    "training_examples = 10000\n",
    "\n",
    "# We'll be using the spaCy's English and German tokenizers\n",
    "spacy_en = English()\n",
    "spacy_de = German()\n",
    "\n",
    "en_words = Counter()\n",
    "de_words = Counter()\n",
    "en_inputs = []\n",
    "de_inputs = []\n",
    "\n",
    "# Tokenizing the English and German sentences and creating our word banks for both languages\n",
    "for i in tqdm(range(training_examples)):\n",
    "    en_tokens = spacy_en(en[i])\n",
    "    de_tokens = spacy_de(de[i])\n",
    "    if len(en_tokens)==0 or len(de_tokens)==0:\n",
    "        continue\n",
    "    for token in en_tokens:\n",
    "        en_words.update([token.text.lower()])\n",
    "    en_inputs.append([token.text.lower() for token in en_tokens] + ['_EOS'])\n",
    "    for token in de_tokens:\n",
    "        de_words.update([token.text.lower()])\n",
    "    de_inputs.append([token.text.lower() for token in de_tokens] + ['_EOS'])\n",
    "\n",
    "# Assigning an index to each word token, \n",
    "# including the Start Of String(SOS), End Of String(EOS) and Unknown(UNK) tokens\n",
    "en_words = ['_SOS','_EOS','_UNK'] + sorted(en_words,key=en_words.get,reverse=True)\n",
    "en_w2i = {o:i for i,o in enumerate(en_words)}\n",
    "en_i2w = {i:o for i,o in enumerate(en_words)}\n",
    "de_words = ['_SOS','_EOS','_UNK'] + sorted(de_words,key=de_words.get,reverse=True)\n",
    "de_w2i = {o:i for i,o in enumerate(de_words)}\n",
    "de_i2w = {i:o for i,o in enumerate(de_words)}\n",
    "\n",
    "# Converting our English and German sentences to their token indexes\n",
    "for i in range(len(en_inputs)):\n",
    "    en_sentence = en_inputs[i]\n",
    "    de_sentence = de_inputs[i]\n",
    "    en_inputs[i] = [en_w2i[word] for word in en_sentence]\n",
    "    de_inputs[i] = [de_w2i[word] for word in de_sentence]\n",
    "\n",
    "print(\"Examples:\")\n",
    "print(\"x:\",en[9002],\"->\",en_inputs[9002],\"|\",\"y:\",de[9002],\"->\",de_inputs[9002])\n",
    "print(\"x:\",en[9003],\"->\",en_inputs[9003],\"|\",\"y:\",de[9003],\"->\",de_inputs[9003])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will be defining our Encoder and Bahdanau Attention Decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model architecture\n",
    "\n",
    "Let's look at the overview of the attention architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./bahdanau.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "### Recurrent network\n",
    "As you can see in the figure above, in the encoder part of the model (the green boxes), one hidden state is produced for each time sample that is fed to the encoder model. The produced hidden state is then used together with the next input time sample to produce the next hidden state and so on until the last input time sample has been fed to the encoder.\n",
    "In other words, after a sequence of $T$ time samples $\\{x_t\\}_1^T$ is fed to the encoder, a sequence of $T$ hidden states $\\{h_t\\}_1^T$ is produced.\n",
    "\n",
    "These architectures are refered to as recurrent architecture and can be found in pytorch recurrent modules, for instance `nn.RNN`, `nn.LSTM` or `nn.GRU`.\n",
    "Here we use the `nn.LSTM`.\n",
    "\n",
    "\n",
    "### Embedding\n",
    "As you saw at the output of the data preparation part, the input data are discrete.\n",
    "\n",
    "This usually no problem, as long the data can be represented in a ordinal scale, but here there is no notion of distance in the data. For instance we cannot say that the english word represented by the index `141` is \"closer\" to the word at index `140` compared to the word at index `120` because `141` is closer to `140`.\n",
    "\n",
    "This is a problem for neural networks which aim at learning structure in the data. To solve this problem we embed our input data into a space where such a notion of distance exists. This is done using `nn.Embedding` layers in pytorch.\n",
    "In fact, these embedding layers are simply trainable linear transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1, drop_prob=0):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers, dropout=drop_prob, batch_first=True)\n",
    "\n",
    "    def forward(self, inputs, hidden,verbose=False):\n",
    "        if verbose:\n",
    "            print(\"inputs:\", inputs.shape)\n",
    "            print(\"hidden:\", [h.shape for h in hidden])\n",
    "            \n",
    "        # Embbed input words\n",
    "        embedded = self.embedding(inputs)\n",
    "        if verbose:\n",
    "            print(\"embedded:\", embedded.shape)\n",
    "            \n",
    "        # Pass the embedded word vectors into LSTM and return all outputs\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        if verbose:\n",
    "            print(\"output\",output.shape)\n",
    "            print(\"hidden\",[h.shape for h in hidden])\n",
    "            \n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return (torch.zeros(self.n_layers, batch_size, self.hidden_size, device=device),\n",
    "                torch.zeros(self.n_layers, batch_size, self.hidden_size, device=device))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder: the Bahdanau attention mechanism\n",
    "\n",
    "The main idea is to use information from all the intermediate outputs of the encoder in the decoder.\n",
    "This done by creating a `context` vector at each decoding time steps.\n",
    "The role of the context vector is to weight information scattered accross the Encoder's hidden state sequence.\n",
    "You will find a flow chart for the decoder forward pass below.\n",
    "\n",
    "At each decoding time step, the previous decoder output and a context vector are aggregated and used as input to the reccurent model (step 6).\n",
    "The context vector has the dimension of the hidden states, and is calculated as a weighted average of the sequence of encoder outputs (step 5).\n",
    "The weights are obtained from a non-linear, trainable transform of (1) the previous decoder hidden state and (2) the sequence of encoder's output (step 1, 2, 3 & 4).\n",
    "\n",
    "![](./context.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Task: Implement step 1, 2 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, drop_prob=0.1):\n",
    "        super(BahdanauDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        \n",
    "        # The role of the embedding layer is to encode the word indexes into a dense vector space.\n",
    "        # The goal is to learn semantic relations between word in a sentence.\n",
    "        # More Information about how the embedding layers \n",
    "        # can be found here https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "\n",
    "        self.fc_hidden = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.fc_encoder = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "    \n",
    "        # Randomly zeros some inputs with probability self.drop_prob\n",
    "        # More information can be found in the original research paper: https://arxiv.org/abs/1207.0580\n",
    "        self.dropout = nn.Dropout(self.drop_prob)\n",
    "        self.lstm = nn.LSTM(self.hidden_size*2, self.hidden_size, batch_first=True)\n",
    "        self.classifier = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, inputs, hidden, encoder_outputs, verbose=False):\n",
    "        if verbose:\n",
    "            print(\"Input:\",inputs.shape)\n",
    "            print(\"hidden & cell:\",[h.shape for h in hidden])\n",
    "            print(\"encoder_outputs:\", encoder_outputs.shape)\n",
    "            \n",
    "        encoder_outputs = encoder_outputs.squeeze()\n",
    "        if verbose:\n",
    "            print(\"encoder_outputs.squeeze():\", encoder_outputs.shape)\n",
    "\n",
    "        embedded = self.embedding(inputs).view(1, -1)    \n",
    "        embedded = self.dropout(embedded)\n",
    "        if verbose:\n",
    "            print(\"embedded:\",embedded.shape)\n",
    "\n",
    "        # Calculating Alignment Scores\n",
    "        # Steps 1 & 2:  Merge the information from the hidden state & the encoder's output\n",
    "        # x should process the data according to the flowchart and have size (1 x T x hidden_size)\n",
    "        x = torch.tanh(self.fc_hidden(hidden[0])+self.fc_encoder(encoder_outputs))\n",
    "        if verbose:\n",
    "            print(\"\\nStep 1 & 2\")\n",
    "            print(\"x:\",x.shape)\n",
    "        \n",
    "        # Step 3: Multiply by trainable weights\n",
    "        # alignment_scores should be of size (1 x T x 1)\n",
    "        # self.weight has size (1 x 256)\n",
    "        alignment_scores = x.bmm(self.weight.unsqueeze(2))\n",
    "        if verbose:\n",
    "            print(\"\\nStep 3\")\n",
    "            print(\"self.weight:\",self.weight.shape)\n",
    "            print(\"self.weight.unsqueeze(2):\",self.weight.unsqueeze(2).shape)\n",
    "            print(\"alignment_scores:\",alignment_scores.shape)\n",
    "        \n",
    "        # Step 4: Softmaxing alignment scores to get Attention weights\n",
    "        # attn_weights should have size (1 x 3)\n",
    "        attn_weights = F.softmax(alignment_scores.view(1,-1), dim=1)\n",
    "        if verbose:\n",
    "            print(\"\\nStep 4\")\n",
    "            print(\"attn_weights:\",attn_weights.shape)\n",
    "            print(\"attn_weights.unsqueeze(0):\",attn_weights.unsqueeze(0).shape)\n",
    "        \n",
    "        # Step 5: Multiplying the Attention weights with encoder outputs to get the context vector\n",
    "        context_vector = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\nStep 5\")\n",
    "            print(\"context_vector:\",context_vector.shape)\n",
    "        \n",
    "        # Step 6:  Concatenating context vector with embedded input word\n",
    "        output = torch.cat((embedded, context_vector[0]), 1).unsqueeze(0)\n",
    "        if verbose:\n",
    "            print(\"\\nStep 6\")\n",
    "            print(\"output:\", output.shape)\n",
    "        \n",
    "        # Passing the concatenated vector as input to the LSTM cell\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        if verbose:\n",
    "            print(\"output:\", output.shape)\n",
    "            print(\"hidden:\", [h.shape for h in hidden])\n",
    "\n",
    "        # Passing the LSTM output through a Linear layer acting as a classifier\n",
    "        output = F.log_softmax(self.classifier(output[0]), dim=1)\n",
    "        \n",
    "        return output, hidden, attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  A test run\n",
    "\n",
    "We set some `hidden_size`, instanciate the models and make a forward pass to test the dimension of the intermediate variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder\n",
      "inputs: torch.Size([1, 3])\n",
      "hidden: [torch.Size([1, 1, 256]), torch.Size([1, 1, 256])]\n",
      "embedded: torch.Size([1, 3, 256])\n",
      "output torch.Size([1, 3, 256])\n",
      "hidden [torch.Size([1, 1, 256]), torch.Size([1, 1, 256])]\n",
      "\n",
      "Decoder\n",
      "\n",
      "\n",
      "sample: 1 \n",
      "\n",
      "Input: torch.Size([1])\n",
      "hidden & cell: [torch.Size([1, 1, 256]), torch.Size([1, 1, 256])]\n",
      "encoder_outputs: torch.Size([1, 3, 256])\n",
      "encoder_outputs.squeeze(): torch.Size([3, 256])\n",
      "embedded: torch.Size([1, 256])\n",
      "\n",
      "Step 1 & 2\n",
      "x: torch.Size([1, 3, 256])\n",
      "\n",
      "Step 3\n",
      "self.weight: torch.Size([1, 256])\n",
      "self.weight.unsqueeze(2): torch.Size([1, 256, 1])\n",
      "alignment_scores: torch.Size([1, 3, 1])\n",
      "\n",
      "Step 4\n",
      "attn_weights: torch.Size([1, 3])\n",
      "attn_weights.unsqueeze(0): torch.Size([1, 1, 3])\n",
      "\n",
      "Step 5\n",
      "context_vector: torch.Size([1, 1, 256])\n",
      "\n",
      "Step 6\n",
      "output: torch.Size([1, 1, 512])\n",
      "output: torch.Size([1, 1, 256])\n",
      "hidden: [torch.Size([1, 1, 256]), torch.Size([1, 1, 256])]\n",
      "\n",
      "\n",
      "sample: 2 \n",
      "\n",
      "Input: torch.Size([1])\n",
      "hidden & cell: [torch.Size([1, 1, 256]), torch.Size([1, 1, 256])]\n",
      "encoder_outputs: torch.Size([1, 3, 256])\n",
      "encoder_outputs.squeeze(): torch.Size([3, 256])\n",
      "embedded: torch.Size([1, 256])\n",
      "\n",
      "Step 1 & 2\n",
      "x: torch.Size([1, 3, 256])\n",
      "\n",
      "Step 3\n",
      "self.weight: torch.Size([1, 256])\n",
      "self.weight.unsqueeze(2): torch.Size([1, 256, 1])\n",
      "alignment_scores: torch.Size([1, 3, 1])\n",
      "\n",
      "Step 4\n",
      "attn_weights: torch.Size([1, 3])\n",
      "attn_weights.unsqueeze(0): torch.Size([1, 1, 3])\n",
      "\n",
      "Step 5\n",
      "context_vector: torch.Size([1, 1, 256])\n",
      "\n",
      "Step 6\n",
      "output: torch.Size([1, 1, 512])\n",
      "output: torch.Size([1, 1, 256])\n",
      "hidden: [torch.Size([1, 1, 256]), torch.Size([1, 1, 256])]\n",
      "\n",
      "\n",
      "sample: 3 \n",
      "\n",
      "Input: torch.Size([1])\n",
      "hidden & cell: [torch.Size([1, 1, 256]), torch.Size([1, 1, 256])]\n",
      "encoder_outputs: torch.Size([1, 3, 256])\n",
      "encoder_outputs.squeeze(): torch.Size([3, 256])\n",
      "embedded: torch.Size([1, 256])\n",
      "\n",
      "Step 1 & 2\n",
      "x: torch.Size([1, 3, 256])\n",
      "\n",
      "Step 3\n",
      "self.weight: torch.Size([1, 256])\n",
      "self.weight.unsqueeze(2): torch.Size([1, 256, 1])\n",
      "alignment_scores: torch.Size([1, 3, 1])\n",
      "\n",
      "Step 4\n",
      "attn_weights: torch.Size([1, 3])\n",
      "attn_weights.unsqueeze(0): torch.Size([1, 1, 3])\n",
      "\n",
      "Step 5\n",
      "context_vector: torch.Size([1, 1, 256])\n",
      "\n",
      "Step 6\n",
      "output: torch.Size([1, 1, 512])\n",
      "output: torch.Size([1, 1, 256])\n",
      "hidden: [torch.Size([1, 1, 256]), torch.Size([1, 1, 256])]\n",
      "[326, 2828, 2832]\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "isample = 0\n",
    "encoder = EncoderLSTM(len(en_words), hidden_size).to(device)\n",
    "decoder = BahdanauDecoder(hidden_size,len(de_words)).to(device)\n",
    "\n",
    "print(\"Encoder\")\n",
    "h = encoder.init_hidden()\n",
    "inp = torch.tensor(en_inputs[isample]).unsqueeze(0).to(device)\n",
    "encoder_outputs, h = encoder(inp,h,verbose=True)\n",
    "\n",
    "# First decoder input will be the SOS token (Start of String)\n",
    "decoder_input = torch.tensor([en_w2i['_SOS']],device=device)\n",
    "\n",
    "# First decoder hidden state will be last encoder hidden state\n",
    "decoder_hidden = h\n",
    "output = []\n",
    "\n",
    "print(\"\\nDecoder\")\n",
    "# For all the words in the sentence\n",
    "for ii in range(len(de_inputs[isample])):\n",
    "    print(\"\\n\\nsample:\", ii+1, \"\\n\")\n",
    "    decoder_output, decoder_hidden,attn_weights = decoder(decoder_input, decoder_hidden, encoder_outputs, verbose=True)\n",
    "\n",
    "    # Get the index value of the word with the highest score from the decoder output\n",
    "    top_value, top_index = decoder_output.topk(1)\n",
    "\n",
    "    # Use the decoder's output as the next input\n",
    "    decoder_input = torch.tensor([top_index.item()],device=device)\n",
    "\n",
    "    # Save the predicted german word\n",
    "    output.append(top_index.item())\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "We can now set the `hidden_size` and `learning_rate (lr)` hyperparameters and instantiate the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder = EncoderLSTM(len(en_words), hidden_size).to(device)\n",
    "decoder = BahdanauDecoder(hidden_size,len(de_words)).to(device)\n",
    "\n",
    "lr = 0.001\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
    "\n",
    "EPOCHS = 10 #(on 4 cpus ~15min per epoch)\n",
    "teacher_forcing_prob = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During our training cycle, weâ€™ll be using a method called **teacher forcing**.\n",
    "\n",
    "As we saw earlier, the role of the decoder model is to predict the sequence of upcoming words.\n",
    "At testing time, the seq2seq model does not return the whole predicted sequence at once, it returns a predicted sequence iteratively. This means that the model uses its own predicted word as input in order to predict the next word.\n",
    "At training time, the decoder's prediction is performed similarly. We however have the choice of using either the true word or the predicted word.\n",
    "Teacher forcing is a method used at training time, and implements a switch at the input of a decoder model.\n",
    "With probability $p$, the switch will select the true previous word as input to predict the next.\n",
    "With probability $1-p$ the switch will select the previous predicted word as input to predict the next.\n",
    "\n",
    "\n",
    "- Task: Finish the implementation of the training algorithm\n",
    "- Task: According to you, why is using teacher forcing a good idea ? What advantages and drawbacks do you see with this method ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "One advantage of having a decoder which works on it's own predictions is that the training procedure ressembles the testing procedure. This is always better since the trained model then has a chance to perform well also on test cases. \n",
    "However, when learning to predict a sequence, the model training is usually difficult in the sense that errors made early in the sequence prediction propagate to the next time steps.\n",
    "Therefore, a hybrid solution where the decoder (the student) can use the true value it tries to predict (the teacher) during training makes training faster while maintain the generalizability of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Task: Finish the implementation of the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bad2f843aa447889b1197d1fdc5f1ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Progression bar for the number of epochs\n",
    "tk0 = tqdm(range(1,EPOCHS+1),total=EPOCHS)\n",
    "EPOCHS=5\n",
    "for epoch in tk0:\n",
    "    avg_loss = 0.\n",
    "    \n",
    "    # Progression bar for the iteration accross the training dataset\n",
    "    tk1 = tqdm(enumerate(en_inputs),total=len(en_inputs),leave=False)\n",
    "    for i, sentence in tk1:\n",
    "        # Initialize for a new sentence\n",
    "        loss = 0.\n",
    "        h = encoder.init_hidden()\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        \n",
    "        inp = torch.tensor(sentence).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Encode the sentence, encoder_outputs will not be used \n",
    "        encoder_outputs, h = encoder(inp,h)\n",
    "        \n",
    "\n",
    "        # First decoder input will be the SOS token (Start of String)\n",
    "        decoder_input = torch.tensor([en_w2i['_SOS']],device=device)\n",
    "        \n",
    "        # First decoder hidden state will be last encoder hidden state\n",
    "        decoder_hidden = h\n",
    "        output = []\n",
    "        \n",
    "        # Choose whether the current sequence will be learnt with teacher forcing or not\n",
    "        teacher_forcing = random.random() < teacher_forcing_prob\n",
    "        \n",
    "        # For all the words in the sentence\n",
    "        for ii in range(len(de_inputs[i])):\n",
    "            decoder_output, decoder_hidden, attn_weights = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            # Get the index value of the word with the highest score from the decoder output\n",
    "            top_value, top_index = decoder_output.topk(1)\n",
    "            \n",
    "            if teacher_forcing:\n",
    "                # Use the true german word as the next input to the decoder\n",
    "                decoder_input = torch.tensor([de_inputs[i][ii]],device=device)\n",
    "            else:\n",
    "                # Use the previous decoder output as the next input to the decoder \n",
    "                decoder_input = torch.tensor([top_index.item()],device=device)\n",
    "            \n",
    "            # Save the predicted german word\n",
    "            output.append(top_index.item())\n",
    "            \n",
    "            # Calculate the loss of the prediction against the actual word\n",
    "            loss += F.nll_loss(decoder_output.view(1,-1), torch.tensor([de_inputs[i][ii]],device=device))\n",
    "        \n",
    "        # Average the loss of the individual word at the end of the sentence\n",
    "        loss = loss/len(de_inputs[i])\n",
    "        \n",
    "        # The encoder and decoder gradients are back-propagated from the same loss\n",
    "        # Write the end of the training loop here:\n",
    "        # ...\n",
    "        # ...\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        \n",
    "        avg_loss += loss.item()/len(en_inputs)\n",
    "    \n",
    "    tk0.set_postfix(loss=avg_loss)\n",
    "\n",
    "# Save model after training (Optional)\n",
    "torch.save({\"encoder\":encoder.state_dict(),\n",
    "            \"decoder\":decoder.state_dict(),\n",
    "            \"e_optimizer\":encoder_optimizer.state_dict(),\n",
    "            \"d_optimizer\":decoder_optimizer.state_dict()},\"./model.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results visualization\n",
    "\n",
    "Using our trained model, letâ€™s visualise some of the outputs that the model produces and the attention weights the model assigns to each input element.\n",
    "\n",
    "Here your task is to:\n",
    "- Task: find a way to create a list of the attention weights, along with the list of predicted words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: it was nothing . _EOS\n",
      "Predicted: es war nichts .\n",
      "Actual: nicht der rede wert ! _EOS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_494956/3275040060.py:35: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels(['']+[en_i2w[x] for x in en_inputs[i]])\n",
      "/tmp/ipykernel_494956/3275040060.py:36: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels(['']+[de_i2w[x] for x in output])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Choose a random sentences\n",
    "    i = random.randint(0,len(en_inputs)-1)\n",
    "    h = encoder.init_hidden()\n",
    "    inp = torch.tensor(en_inputs[i]).unsqueeze(0).to(device)\n",
    "    encoder_outputs, h = encoder(inp,h)\n",
    "\n",
    "    decoder_input = torch.tensor([en_w2i['_SOS']],device=device)\n",
    "    decoder_hidden = h\n",
    "    output = []\n",
    "    attentions = []\n",
    "    while True:\n",
    "        decoder_output, decoder_hidden, attn_weights = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "        _, top_index = decoder_output.topk(1)\n",
    "        decoder_input = torch.tensor([top_index.item()],device=device)\n",
    "        \n",
    "        #If the decoder output is the End Of Sentence token, stop decoding process\n",
    "        if top_index.item() == de_w2i[\"_EOS\"]:\n",
    "            break\n",
    "        output.append(top_index.item())\n",
    "        attentions.append(attn_weights.squeeze().cpu().detach().numpy())\n",
    "\n",
    "print(\"English: \"+ \" \".join([en_i2w[x] for x in en_inputs[i]]))\n",
    "print(\"Predicted: \" + \" \".join([de_i2w[x] for x in output]))\n",
    "print(\"Actual: \" + \" \".join([de_i2w[x] for x in de_inputs[i]]))\n",
    "\n",
    "# Plotting the heatmap for the Attention weights\n",
    "fig = plt.figure(figsize=(12,9))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(np.array(attentions))\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels(['']+[en_i2w[x] for x in en_inputs[i]])\n",
    "ax.set_yticklabels(['']+[de_i2w[x] for x in output])\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the attention mechanism\n",
    "\n",
    "You should obtain a figure where:\n",
    "- Each row corresponds to an output word in german\n",
    "- Each column corresponds to an input word in english.\n",
    "- The pixel intensity corresponds to the weight of the input word in the output word prediction assigned by the attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks\n",
    "\n",
    "From the example above, we can see that for each output word from the decoder, the weights assigned to the input words are different and we can see the relationship between the inputs and outputs that the model is able to draw. You can try this on a few more examples to test the results of the translator.\n",
    "\n",
    "In our training, we have clearly overfitted our model to the training sentences. If we were to test the trained model on sentences it has never seen before, it is unlikely to produce decent results. Nevertheless this process acts as a sanity check to ensure that our model works and is able to function end-to-end and learn.\n",
    "\n",
    "The challenge of training an effective model can be attributed largely to the lack of training data and training time. Due to the complex nature of the different languages involved and large number of vocabulary and grammatical permutations, an effective model will require tons of data and training time before any results can be seen on evaluation data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning2022",
   "language": "python",
   "name": "deeplearning2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
