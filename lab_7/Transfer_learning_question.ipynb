{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f03d341",
   "metadata": {
    "id": "3f03d341"
   },
   "source": [
    "# Transfer Learning\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this tutorial, we will look at classifying images from a dataset by using transfer learning from a pre-trained network. In our case, we will use resnet-18. You will try two ways to customize a pretrained model:\n",
    "\n",
    "1. Feature Extraction: Use the representations learned by a previous network to extract meaningful features from new samples. You simply add a new classifier, which will be trained from scratch, on top of the pretrained model so that you can repurpose the feature maps learned previously for the dataset.\n",
    "\n",
    "2. Fine-Tuning: We use the pre-trained model as a starting point and then train not only a new classifier on top of the pretrained model, but also continue to backpropagate and then train the total network. \n",
    "\n",
    "## Dataset\n",
    "\n",
    "We will use the Hymenoptera dataset of ants and bees from this [link](https://www.kaggle.com/datasets/ajayrana/hymenoptera-data) for our transfer learning tutorial. Download the zip file and extract the files. Each category ('ants' / 'bees') consists of 120 images for training and 75 images for validation. The dataset is a small subset of the Imagenet dataset.\n",
    "\n",
    "If you plan to use Google Colab, you can find ways to download the dataset from Kaggle and upload it to your Colab notebook. One such way is [here](https://medium.com/analytics-vidhya/how-to-download-kaggle-datasets-into-google-colab-via-google-drive-dcb348d7af07).\n",
    "\n",
    "## Task\n",
    "\n",
    "In this notebook, you will be\n",
    "\n",
    "- Loading the new dataset and pre-processing it for training\n",
    "- Loading a pretrained base model\n",
    "- Fine-tuning the base model on the new dataset and examining its performance\n",
    "- Finally use the base model as a feature extractor and again examining its performance\n",
    "\n",
    "Just like in the previous notebooks, most of the exercises include completing missing code segments. Please got through the notebook sequentially and find the relevant instructions in the code cells / markdown above the code cells. Also, as always, feel free to add comments beside your code for ease of understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bb4a36",
   "metadata": {
    "id": "d7bb4a36"
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from datetime import datetime\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torchvision.datasets import ImageFolder, DatasetFolder\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3645ad5c",
   "metadata": {
    "id": "3645ad5c"
   },
   "source": [
    "### Pre-processing the dataset\n",
    "\n",
    "[**datasets.ImageFolder**](https://pytorch.org/vision/main/generated/torchvision.datasets.ImageFolder.html) basically provides a data structure where these images are arranged such that:\n",
    "\n",
    "`data_dir/train/ants/x.png,`\n",
    "\n",
    "`data_dir/train/ants/y.png,`\n",
    "\n",
    "`data_dir/train/ants/z.png,`\n",
    "\n",
    "`data_dir/train/ants/w.png ...`\n",
    "\n",
    "Note that, the above organization is an example structure, where `data_dir` represents the exact location where you're extracted dataset is present. Inside the folder you will have two folders `train` and `val` containing training and validation set images respectively. Additionally, specific transforms can be used on the input images using `torchvision.transforms`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RDIcIyxdBDaW",
   "metadata": {
    "id": "RDIcIyxdBDaW"
   },
   "source": [
    "Task: Create a dictionary `image_datasets` that consists of the training and validation datasets under two keys `train` and `val` respectively. Remember training and validation datasets are stored under `data_dir/train/...` and `data_dir/val/...` respectively. Load the datasets using `datasets.ImageFolder` and specify appropriate transforms from the dictionary `data_transforms` which is already given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e70e96c",
   "metadata": {
    "id": "1e70e96c"
   },
   "outputs": [],
   "source": [
    "# Set dataset related parameters here:\n",
    "BATCH_SIZE = 64 # Batch size for the data loader\n",
    "train_to_val_split = 0.8 # Percentage of the validation set to be used as the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bOMKhwoeBM1d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "bOMKhwoeBM1d",
    "outputId": "c17bbf26-508a-4dd4-bc71-afa41fa712eb"
   },
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = None # Specify the path where you loaded your data\n",
    "image_datasets = None # Create this dictionary to load your datasets as per the \n",
    "                      # instructions provided above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7042fbd",
   "metadata": {
    "id": "c7042fbd"
   },
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"\n",
    "    Defining a function to display images from the hymenoptera dataset\n",
    "    \"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943aa506",
   "metadata": {
    "id": "943aa506"
   },
   "source": [
    "## Make sure dataset is loaded correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a03d85",
   "metadata": {
    "id": "b0a03d85"
   },
   "source": [
    "### Create an additional test dataset by partitioning the validation set\n",
    "\n",
    "We create a test dataset by using the validation set since the original dataset was only partitioned into training and validation datasets. For validation, we use a part of the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5580a35",
   "metadata": {
    "id": "e5580a35"
   },
   "outputs": [],
   "source": [
    "def create_val_and_test_loaders(image_datasets, train_to_val_split):\n",
    "    \n",
    "    # Create the dataloaders \n",
    "\n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "    class_names = image_datasets['train'].classes\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Split the training dataset as per 'train_to_val_split' into training and validation sets\n",
    "    num_train_plus_val_samples = dataset_sizes['train']\n",
    "    indices_train_plus_val_permuted = torch.randperm(num_train_plus_val_samples)\n",
    "    num_train_samples = int(train_to_val_split * num_train_plus_val_samples)\n",
    "    num_val_samples = num_train_plus_val_samples - num_train_samples\n",
    "    idx_train = indices_train_plus_val_permuted[:num_train_samples]\n",
    "    idx_val = indices_train_plus_val_permuted[num_train_samples:]\n",
    "\n",
    "    # Recreate the validation dataset loader and test dataset loader\n",
    "\n",
    "    dataloaders['train'] = torch.utils.data.DataLoader(image_datasets['train'], batch_size=BATCH_SIZE,\n",
    "                                                     sampler=SubsetRandomSampler(idx_train),\n",
    "                                                     num_workers=2)\n",
    "    \n",
    "    dataloaders['val'] = torch.utils.data.DataLoader(image_datasets['train'], batch_size=BATCH_SIZE,\n",
    "                                                     sampler=SubsetRandomSampler(idx_val),\n",
    "                                                     num_workers=2)\n",
    "\n",
    "    dataloaders['test'] = torch.utils.data.DataLoader(image_datasets['val'], batch_size=BATCH_SIZE,\n",
    "                                                     shuffle=True,\n",
    "                                                     num_workers=2)\n",
    "    \n",
    "    return dataloaders, dataset_sizes, device, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64cebd0",
   "metadata": {
    "id": "d64cebd0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataloaders, dataset_sizes, device, class_names = create_val_and_test_loaders(image_datasets, train_to_val_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8ae5f5",
   "metadata": {
    "id": "2b8ae5f5"
   },
   "source": [
    "### Examining some of the images of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a225deb4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "a225deb4",
    "outputId": "03aa56d2-7801-4a88-bbc7-7ee0da741937"
   },
   "outputs": [],
   "source": [
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs[:6], nrow=6)\n",
    "imshow(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5953805e",
   "metadata": {
    "id": "5953805e"
   },
   "source": [
    "### Defining the function to train the model\n",
    "\n",
    "Task: Complete the function `evaluate_model` to test the trained model on the dataloader specified by `mode` (`mode`can be 'test' / 'val'). Note that here no gradients should be computed for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LzC0HJ1kF-P5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "LzC0HJ1kF-P5",
    "outputId": "7e968413-b707-40cc-b617-44f0d0f58796"
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(trained_model, dataloaders, mode='test', device='cpu'):\n",
    "    \n",
    "    num_total = 0\n",
    "    num_correct = 0\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # ... Set the trained_model in 'eval' mode\n",
    "    \n",
    "    with None: # Insert code here to use an environment that avoids computation of \n",
    "    # gradients. You can use the no_grad environment by PyTorch\n",
    "    \n",
    "        for _, test_data in enumerate(dataloaders[mode]):\n",
    "            \n",
    "            test_images, test_labels = test_data\n",
    "            test_images = None # Push the inputs to the device\n",
    "            test_labels = None # Push the labels to the device \n",
    "            test_outputs = None # Pass the inputs through the model\n",
    "\n",
    "            test_predicted = None # Get the predicted labels from the test outputs. \n",
    "            # Usually the test_outputs represent a softmax output\n",
    "\n",
    "            num_total += test_labels.size(0)\n",
    "            num_correct += (test_predicted == test_labels).sum().item()\n",
    "            \n",
    "    time_elapsed = (datetime.now() - start_time).total_seconds()\n",
    "    eval_acc = 100 * num_correct / num_total # Calculating the accuracy on the evaluation set in percentage\n",
    "    \n",
    "    print(\"Accuracy of the network on the {} images: {} %\".format(\n",
    "        mode, eval_acc))\n",
    "    print(\"Evaluation runtime: {:.3f} secs\".format(time_elapsed))\n",
    "    \n",
    "    return eval_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SpHQ-Tq7HPOd",
   "metadata": {
    "id": "SpHQ-Tq7HPOd"
   },
   "source": [
    "## Training the model\n",
    "\n",
    "Task: Complete parts of the code for training the base model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2IdIAHGRHa_A",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "2IdIAHGRHa_A",
    "outputId": "756b10f9-0848-4a2f-eb63-0f334502fc41"
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, lr_scheduler, num_epochs=10, device='cpu'):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    best_model_wts = None\n",
    "    best_validation_acc = 0.0\n",
    "    tr_running_loss = 0.0\n",
    "    tr_loss_epoch_sum = 0.0\n",
    "    train_total = 0.0 \n",
    "    train_correct = 0.0\n",
    "    best_validation_epoch = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        # Iterate over data.\n",
    "        for i, tr_data in enumerate(dataloaders['train'],0):\n",
    "            \n",
    "            tr_inputs, tr_labels = tr_data\n",
    "            tr_inputs = tr_inputs.to(device) \n",
    "            tr_labels = tr_labels.to(device)\n",
    "            \n",
    "            ... # zero-out optimizer gradients\n",
    "            tr_outputs = ... # get the model outputs on the training inputs\n",
    "            tr_loss = ... # Compute the loss function using the model outputs and criterion\n",
    "            ... # Compute the gradients\n",
    "            ... # Update the weights using the optimizer\n",
    "\n",
    "            # statistics\n",
    "            tr_loss_epoch_sum += tr_loss.item()\n",
    "            tr_running_loss += tr_loss.item() #* inputs.size(0)\n",
    "            \n",
    "            if i % 5 == 4:\n",
    "                # print progress statistics\n",
    "                print(\"Epoch: {:d}/{:d}, batch_no.: {:5d}, loss: {:.3f}\".format(\n",
    "                    epoch + 1, num_epochs, i+1, tr_running_loss / 5\n",
    "                ))\n",
    "                tr_running_loss = 0.0\n",
    "            \n",
    "            _, tr_predicted = torch.max(tr_outputs, 1)\n",
    "            train_total += tr_labels.size(0)\n",
    "            train_correct += (tr_predicted == tr_labels).sum().item()\n",
    "        \n",
    "        tr_loss_epoch = tr_loss / len(dataloaders['train'])\n",
    "        tr_acc_epoch = 100 * train_correct / train_total\n",
    "        \n",
    "        lr_scheduler.step() # This helps to adaptively decrease the learning rate\n",
    "        \n",
    "        val_acc_epoch = None # Get the accuracy on the validation set by calling the evaluate_model function \n",
    "        \n",
    "        print(\"Tr. loss: {:.3f}, Training set accuracy: {:.3f}, Validation set accuracy: {:.3f}\".format(\n",
    "                    tr_loss_epoch, tr_acc_epoch, val_acc_epoch))\n",
    "        \n",
    "        # deep copy the model\n",
    "        if val_acc_epoch > best_validation_acc:\n",
    "            best_validation_acc = val_acc_epoch\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            best_validation_epoch = epoch + 1\n",
    "\n",
    "    time_elapsed = time.time() - start_time\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_validation_acc))\n",
    "    print(\"Saving model at Epoch: {} ...\".format(best_validation_epoch))\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O6IPNL9iHdqr",
   "metadata": {
    "id": "O6IPNL9iHdqr"
   },
   "source": [
    "## Visualizing some of the predictions of the model\n",
    "\n",
    "The `visualize_model` function takes some images from the validation set and evaluates the trained model on that set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdd6ed9",
   "metadata": {
    "id": "9cdd6ed9"
   },
   "outputs": [],
   "source": [
    "def visualize_model(trained_model, num_images=6,  device='cpu'):\n",
    "    \n",
    "    was_training = trained_model.training\n",
    "    trained_model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = trained_model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title(f'predicted: {class_names[preds[j]]}')\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    trained_model.train(mode=was_training)\n",
    "                    return\n",
    "                    \n",
    "        trained_model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JsQQzeZcHoin",
   "metadata": {
    "id": "JsQQzeZcHoin"
   },
   "source": [
    "## Fine-tuning a trained network\n",
    "\n",
    "In this first task, we will fine-tune the weights of pre-trained base-model\n",
    "using the new dataset. This will involve changing the last layer to account for the number of classes of the new dataset. By default the initialization with an nn.Module say nn.Linear sets the layer to have `requires_grad = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gHj_RGpoICtZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "gHj_RGpoICtZ",
    "outputId": "87a4085d-ef94-4c5e-b2bd-c95f397872ce"
   },
   "outputs": [],
   "source": [
    "def change_last_layer(model_ft, num_classes, device):\n",
    "    \"\"\" This function pushes the model to the device 'cpu/gpu' and resets the last layer to include the appropriate number of \n",
    "    classes\n",
    "    \"\"\"\n",
    "    # The last layer is usually a fully connected layer. \n",
    "    num_ftrs = None # Get the number of features of the last fully connected layer by accessing the 'fc' attribute of 'model_ft'\n",
    "    model_ft.fc = None # # Reset the last layer by using a linear layer that uses the same number of \n",
    "    # input features, but different number of output classes\n",
    "    model_ft = None # Push the model to the 'device' \n",
    "    return model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VkmGWs8zJfpg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "VkmGWs8zJfpg",
    "outputId": "90eae32f-7174-46ea-dca2-478440e1c677"
   },
   "outputs": [],
   "source": [
    "model_ft = None # Load a pre-trained resnet18 model from torchvision.models\n",
    "model_ft = None # call the change_last_layer function with proper arguments to return a model whose last layer has been now altered\n",
    "criterion = nn.CrossEntropyLoss() # Define a loss criterion\n",
    "optimizer_ft = torch.optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9) # Set an optimizer\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer_ft, gamma=0.99) # Use an exponential learning rate decaying function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcJu1R5dJuja",
   "metadata": {
    "id": "bcJu1R5dJuja"
   },
   "source": [
    "Training starts for the fine-tuned model. Here we train with a learning rate of 0.001 and for 15 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb335977",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cb335977",
    "outputId": "bfbf203f-888c-4bd8-e14b-0744db2ef88d"
   },
   "outputs": [],
   "source": [
    "model_fine_tuned = train_model(model_ft, dataloaders, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=15, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebc4505",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "id": "eebc4505",
    "outputId": "b18c784f-f8f7-470c-e1ac-d1e1ac989ca9"
   },
   "outputs": [],
   "source": [
    "# Check the visualization on some images from the validation set\n",
    "visualize_model(model_fine_tuned, device=device)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vBiTYmxP7cqw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vBiTYmxP7cqw",
    "outputId": "b4111964-490e-4d9a-e243-8e83b2814abe"
   },
   "outputs": [],
   "source": [
    "test_acc_epoch = evaluate_model(model_fine_tuned, dataloaders, mode='test', device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed52b968",
   "metadata": {
    "id": "ed52b968"
   },
   "source": [
    "## Pre_trained_model as a fixed feature extractor\n",
    "\n",
    "In this part, we use the pretrained base model as a fixed feature extractor. By fixed, we mean that all layers of the base model except the last layer are frozen and no gradients will be computed on them. Gradients are only optimized for the last layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "W3lC9h_5MkNv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "W3lC9h_5MkNv",
    "outputId": "7e29de8c-ef78-473e-914f-2c551058efdc"
   },
   "outputs": [],
   "source": [
    "def freeze_all_layers(model, device='cpu'):\n",
    "    \"\"\"\n",
    "    This function helps freeze all layers of the network and \n",
    "    pushes the model to the correct device\n",
    "    \"\"\"\n",
    "    # Add code to ensure the requires_grad flag of parameters of the model are all set to False.\n",
    "    # You can do this by getting the parameters from model.parameters()\n",
    "    for param in model.parameters():\n",
    "        None # ... Add code here\n",
    "    model = model.to(device) # Pushing the model to the device\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_PE1aQhRuKvL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "_PE1aQhRuKvL",
    "outputId": "58074193-8df7-47b4-b3d7-917d623d7c35"
   },
   "outputs": [],
   "source": [
    "model_conv = torchvision.models.resnet18(pretrained=True) # We again load a base model that is pre-trained\n",
    "model_conv = freeze_all_layers(model_conv, device=device) # We freeze all layers\n",
    "model_conv = change_last_layer(model_conv, num_classes=len(class_names), device=device) # We unfreeze only the last layer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_conv = torch.optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9) # Observe that only parameters of final layer are being optimized as opposed to before.\n",
    "exp_lr_scheduler_conv = torch.optim.lr_scheduler.ExponentialLR(optimizer_conv, gamma=0.99) # Use an exponential learning rate decaying function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984e54b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "984e54b9",
    "outputId": "57386c14-023b-46e1-fd92-8e977265a544"
   },
   "outputs": [],
   "source": [
    "model_conv = train_model(model_conv, dataloaders, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler_conv, num_epochs=15, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d317081f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "id": "d317081f",
    "outputId": "e5464710-d09e-47dc-d44c-3cf8ea0b6fe8"
   },
   "outputs": [],
   "source": [
    "visualize_model(model_conv, device=device)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d32d9a5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2d32d9a5",
    "outputId": "c7f80da2-370c-4cc6-8ad3-20d2a9389e86"
   },
   "outputs": [],
   "source": [
    "test_acc_epoch_conv = evaluate_model(model_conv, dataloaders, mode='test', device=device)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lab_7_transfer_learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
