{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed536058",
   "metadata": {},
   "source": [
    "## **Multiclass Classification using ELMs and kernels** \n",
    "\n",
    "## **Objective**\n",
    "\n",
    "This tutorial demonstrates a multiclass classification using extreme learning machine (ELM) and kernel method. First, you implement ELM by using the least-square method and observe its training and testing accuracy behavior. Second, you compare it with the kernel method when using a linear kernel and investigate the importance of random weights. \n",
    "\n",
    "Note: the Bonus exercises are not mandatory and will not be used as a grading criterion. However, interested students are encouraged to solve them to have better understanding.\n",
    "\n",
    "## **Homeworks**\n",
    "\n",
    "In this notebook, read the code carefully and complete it to do the following:\n",
    "\n",
    "1. Complete incomplete sections of the code to build an extreme learning machine (ELM) for different cases.\n",
    "2. Observe validation accuracy of ELM versus different values of $\\lambda = 10^{-2}, 10^{-1}, 10^{0}, \\cdots, 10^{3}$ in a single figure, and tune these hyperparameters using grid search. Also, tune $n$ in the same manner.\n",
    "3. What is the best value for the hyperparameters?\n",
    "5. Find the testing accuracy of the kernel method when using linear and  Gaussian kernels on the data.\n",
    "6. *Bonus question*: Can you provide an explanation of why the ELM with Gaussian kernels perform better than the linear kernel case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd464b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ca443f",
   "metadata": {},
   "source": [
    "## **Definition of the required functions**\n",
    "\n",
    "**About the dataset:** Here, we define some of the functions that we are going to use later in this example. In this example, we use the Vowel dataset which contains 11 different vowels in English as described below. The speakers are indexed by integers 0-89. (Actually, there are fifteen individual speakers, each saying each vowel six times.) For each utterance, there are ten floating-point input values, with array indices 0-9. The problem is to train the network as well as possible using only on data from \"speakers\" 0-47, and then to test the network on speakers 48-89, reporting the number of correct classifications in the test set. For more information regarding the Vowel dataset refer to [this page](https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Vowel+Recognition+-+Deterding+Data)).\n",
    "\n",
    "**Brief overview of tasks:**\n",
    "First, you implement ELM by using regularized least-square method and observe its the training and testing accuracy. Let us use a matrix notation for convenience and note the training samples by $\\mathbf{X}=[\\mathbf{x}_1, \\cdots, \\mathbf{x}_J] \\in \\mathbb{R}^{P \\times J}$ and their corresponding targets by $\\mathbf{T}=[\\mathbf{t}_1, \\cdots, \\mathbf{t}_J] \\in \\mathbb{R}^{Q \\times J}$. Here, $P$ is the input samples dimension, $Q$ is the target dimension (number of classes), and $J$ is the number of training samples. The training of regularized ELM can be written as \n",
    "\\begin{align}\n",
    "    \\mathbf{W}^{\\star} = \\underset{\\mathbf{W}}{\\arg\\min} ||\\mathbf{T} - \\mathbf{W} \\mathbf{Y}||_F^2 + \\lambda ||\\mathbf{W}||_F^2,\n",
    "\\end{align}\n",
    "where $\\mathbf{Y} = \\mathbf{g}(\\mathbf{R}\\mathbf{X}+\\mathbf{B}) \\in \\mathbb{R}^{n \\times J}$ is the output of the activation function $\\mathbf{g}$ and $n$ is the number of hidden neurons. Note that the weight matrix $\\mathbf{R} \\in \\mathbb{R}^{n \\times P}$ and the bias matrix $\\mathbf{B}=[\\mathbf{b},\\cdots,\\mathbf{b}] \\in \\mathbb{R}^{n \\times J}$ are chosen as an instance of a random distribution. This minimization problem has the following closed-form solution\n",
    "\\begin{align}\n",
    "    \\mathbf{W}^{\\star} = \\mathbf{T} \\mathbf{Y}'(\\mathbf{Y}\\mathbf{Y}' + \\lambda \\mathbf{I})^{-1},\n",
    "\\end{align}\n",
    "Where $\\mathbf{I} \\in \\mathbb{R}^{n \\times n}$ is the identity matrix. The notation $\\mathbf{Y}'$ denotes the transpose of the matrix $\\mathbf{Y}$. Now we define a member method ${\\texttt{LS(Y_train, T_train, lam)}}$ as the least-square function according to the above derivations. We also define ${\\texttt{calculate_accuracy}}$ function to measure the classification accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8a692e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to load the dataset\n",
    "def prepare_dataset(dataset_location=\"./Vowel.mat\"):\n",
    "    \"\"\"\n",
    "    An example of how the dataset looks in practice:\n",
    "    +-------+--------+-------+---------+\n",
    "        | vowel |  word  | vowel |  word   | \n",
    "        +-------+--------+-------+---------+\n",
    "        |  i    |  heed  |  O    |  hod    |\n",
    "        |  I    |  hid   |  C:   |  hoard  |\n",
    "        |  E    |  head  |  U    |  hood   |\n",
    "        |  A    |  had   |  u:   |  who'd  |\n",
    "        |  a:   |  hard  |  3:   |  heard  |\n",
    "        |  Y    |  hud   |       |         |\n",
    "        +-------+--------+-------+---------+\n",
    "    \"\"\"\n",
    "    X = loadmat(dataset_location)[\"featureMat\"]\n",
    "    Y = loadmat(dataset_location)[\"labelMat\"]\n",
    "    X_train, X_test = X[:, :528].astype(np.float32), X[:, 528:].astype(np.float32)\n",
    "    Y_train, Y_test = Y[:, :528].astype(np.float32), Y[:, 528:].astype(np.float32) \n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a418d320",
   "metadata": {},
   "source": [
    "## **Training ELM by using least-square method**\n",
    "\n",
    "We set the number of hidden neurons $n$ and the regularization hyperparameter $\\lambda$, and construct a single layer neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bed192",
   "metadata": {},
   "source": [
    "### Define a function to calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba32ace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(S, T):\n",
    "    # S: predictions\n",
    "    # T: given targets (ground truth)\n",
    "    Y = np.argmax(S, axis=0)\n",
    "    T = np.argmax(T, axis=0)\n",
    "    accuracy = np.sum([Y == T]) / Y.shape[0] # This computes a classification accuracy\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73924100",
   "metadata": {},
   "source": [
    "### Define a class for an Extreme learning machine network\n",
    "\n",
    "- Complete the following class definition for defining an extreme learning machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74a58ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELM(object):\n",
    "    \"\"\"\n",
    "    This defines a class for the extreme learning machine (ELM)\n",
    "    \"\"\"\n",
    "    def __init__(self, n, lam, P):\n",
    "        self.n = n # Number of hidden nodes \n",
    "        self.lam = lam # Initialize the hyperparameter lambda, which is to be tuned\n",
    "        self.Ri = self.initialise_random_matrix(M=n, N=P) # Initialize the random matrix \n",
    "        self.b = self.initialise_random_matrix(M=n, N=1) # Initialize the bias, also randomly\n",
    "        self.W_ls = None # Initially set to None, because we compute it using least squares\n",
    "    \n",
    "    def activation_function(self, Z):\n",
    "        # Here, we basically implement a ReLU( ) activation function\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def initialise_random_matrix(self, M, N):\n",
    "        \"\"\"\n",
    "        Initialize a random matrix that consists of uniformly \n",
    "        distributed values in the range [-1, 1]. Matrix should be \n",
    "        of shape (M, N)\n",
    "        \"\"\"\n",
    "        # Add code here\n",
    "        # ...\n",
    "        return None # Fix this return statement\n",
    "\n",
    "\n",
    "    def LS(self, Y, T):\n",
    "        \"\"\"Solve the optimization problem as regularized least-squares\"\"\"\n",
    "        P = Y.shape[0] # Dimensionality of the input\n",
    "        # Implement the equation T*Y'*inv(Y*Y' + lambda*I) as the least squares solution\n",
    "        W_ls = # ... Add code here\n",
    "        return W_ls\n",
    "    \n",
    "    def train(self, X, T):\n",
    "        \"\"\"\n",
    "        Train the ELM network. Note that here training indicates simply calculating\n",
    "        the matrix W_ls. Since, we have a closed form expression, this amounts to only\n",
    "        calling the relevant method that we have wrote earlier. \n",
    "        \"\"\"\n",
    "        N = X.shape[1] # Number of samples in the data X\n",
    "        \n",
    "        # Pass the input through the linear transform using the input\n",
    "        Zi = np.dot(self.Ri, X) + np.tile(self.b,(1,N))\n",
    "        \n",
    "        # Obtain the value of Y_i by applying the activation function on Z_i \n",
    "        Yi = # ... Add code here \n",
    "        \n",
    "        # Obtain the least squares matrix W_ls\n",
    "        self.W_ls = self.LS(Y=Yi, T=T)\n",
    "    \n",
    "    def evaluate(self, X):\n",
    "        \"\"\"\n",
    "        This function provides predictions using the ELM network, provided that\n",
    "        the network has been already trained, i.e. the train( ) method has been executed\n",
    "        \"\"\"\n",
    "        N = X.shape[1] # Number of samples in the data X\n",
    "        \n",
    "        # Pass the input through the linear transform using the input\n",
    "        Zi = np.dot(self.Ri, X) + np.tile(self.b,(1,N))\n",
    "        \n",
    "        # Obtain the value of Y_i by applying the activation function on Z_i \n",
    "        Yi = self.activation_function(Z=Zi)\n",
    "        \n",
    "        # Get the prediction using the W_ls\n",
    "        T_hat = # ... # Add code here \n",
    "        return T_hat\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1313ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ELM(X_train, T_train, X_val=None, T_val=None, n=2000, lam=1e2):\n",
    "    \"\"\"\n",
    "    This function trains and evaluates the ELM by instantiating the ELM class,\n",
    "    train the network and then evaluate both the training and the validation\n",
    "    data\n",
    "    \"\"\"\n",
    "    elm_net = ELM(n=n, lam=lam, P=X_train.shape[0])\n",
    "    elm_net.train(X=X_train, T=T_train)\n",
    "    T_hat_train = elm_net.evaluate(X=X_train)\n",
    "    T_hat_val = elm_net.evaluate(X=X_val)\n",
    "    acc_train = calculate_accuracy(T_hat_train, T_train)\n",
    "    acc_val = calculate_accuracy(T_hat_val, T_val)\n",
    "    \n",
    "    return acc_train, acc_val, elm_net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fffb1ed",
   "metadata": {},
   "source": [
    "### Get an initial estimate of accuracy from an ELM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63da73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset\n",
    "X_train, X_test, T_train, T_test = prepare_dataset(dataset_location=\"./Vowel.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888add63",
   "metadata": {},
   "outputs": [],
   "source": [
    "P= X_train.shape[0] # Input features\n",
    "Q = T_train.shape[0] # Output label size\n",
    "N_train = X_train.shape[1] # Number of samples in training data\n",
    "N_test = X_test.shape[1] # Number of samples in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3961b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(P, Q, N_train, N_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5925f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tr_val_data(X, T, train_val_split=0.8):\n",
    "    \"\"\"\n",
    "    Splits the data randomly into training and validation data in the ratio 80:20\n",
    "    \"\"\"\n",
    "    N = X.shape[1]\n",
    "    indices = np.random.permutation(N)\n",
    "    num_train_samples = int(train_val_split * len(indices))\n",
    "    num_val_samples = len(indices) - num_train_samples\n",
    "    train_indices = indices[:num_train_samples]\n",
    "    val_indices = indices[num_train_samples:]\n",
    "    X_train, T_train = X[:, train_indices], T[:, train_indices]\n",
    "    X_val, T_val = X[:, val_indices], T[:, val_indices]\n",
    "    print(X_train.shape, T_train.shape, X_val.shape, T_val.shape)\n",
    "    return X_train, T_train, X_val, T_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1e9377",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_to_val_split = 0.8\n",
    "X_train, T_train, X_val, T_val = split_tr_val_data(X=X_train, T=T_train, train_val_split=tr_to_val_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f457b3",
   "metadata": {},
   "source": [
    "### Executing this cell to train the ELM using a randomly chosen $\\lambda$. \n",
    "\n",
    "We note that executing this cell seeral times can lead to differing results, but in general the classification accuracy on average is poor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7b7b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train_star, acc_val_star, elm_net_star = train_ELM(X_train, T_train, X_val, T_val, n=200, lam=1e-14)\n",
    "T_hat_test = elm_net_star.evaluate(X=X_test)\n",
    "acc_test_star = calculate_accuracy(T_hat_test, T_test)\n",
    "print(\"Lambda: {}, Acc_train: {}, Acc_val: {}, Acc_test: {}\".format(1e-2, \n",
    "                                                                  acc_train_star, \n",
    "                                                                  acc_val_star, \n",
    "                                                                  acc_test_star))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1c54db",
   "metadata": {},
   "source": [
    "## Grid search CV: Sweeping over a set of parameters $\\lambda$ to find the ELM with most suited architecture\n",
    "\n",
    "- We reimplement an `ELM_estimator` by subclassing from `BaseEstimator``\n",
    "- This kind of reimplementation is helpful for utilising the grid search CV library of `sklearn`.\n",
    "- Mainly restructuring the train and evaluate functions as: `fit( )`, `predict( )` and `score()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9460552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class ELM_estimator(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, n=2000, lam=1e-5, P=10):\n",
    "        super().__init__()\n",
    "        self.n = n # Number of hidden nodes \n",
    "        self.P = P # Input dimensionality\n",
    "        self.lam = lam # Initialize the hyperparameter lambda, which is to be tuned\n",
    "        self.Ri = self.initialise_random_matrix(M=self.n, N=self.P) # Initialize the random matrix \n",
    "        self.b = self.initialise_random_matrix(M=n, N=1) # Initialize the bias, also randomly\n",
    "        self.W_ls = None # Initially set to None, because we compute it using\n",
    "        \n",
    "    def activation_function(self, Z):\n",
    "         # Here, we basically implement a ReLU( ) activation function\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def initialise_random_matrix(self, M, N):\n",
    "        \"\"\"\n",
    "        Initialize a random matrix that consists of uniformly \n",
    "        distributed values in the range [-1, 1]. Matrix should be \n",
    "        of shape (M, N)\n",
    "        \"\"\"\n",
    "        # Add code here\n",
    "        # ...\n",
    "        return None # Fix this return statement\n",
    "\n",
    "    def LS(self, Y, T):\n",
    "        \"\"\"Solve the optimization problem as regularized least-squares\"\"\"\n",
    "        P = Y.shape[0] # Dimensionality of the input\n",
    "        # Implement the equation T*Y'*inv(Y*Y' + lambda*I) as the least squares solution\n",
    "        W_ls = # ... Add code here\n",
    "        return W_ls\n",
    "    \n",
    "    def fit(self, X, T):\n",
    "        \"\"\"\n",
    "        Train the ELM network. Note that here training indicates simply calculating\n",
    "        the matrix W_ls. Since, we have a closed form expression, this amounts to only\n",
    "        calling the relevant method that we have wrote earlier. \n",
    "        \"\"\"\n",
    "        X, T = X.T, T.T\n",
    "        N = X.shape[1]\n",
    "        \n",
    "        # Pass the input through the linear transform using the input\n",
    "        Zi = np.dot(self.Ri, X) + np.tile(self.b,(1,N))\n",
    "        \n",
    "        # Obtain the value of Y_i by applying the activation function on Z_i \n",
    "        Yi = # ... Add code here \n",
    "        \n",
    "        self.W_ls = self.LS(Y=Yi, T=T)\n",
    "        return self\n",
    "        #T_hat = np.dot(self.W_ls, Yi)\n",
    "        #return T_hat\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        This function provides predictions using the ELM network, provided that\n",
    "        the network has been already trained, i.e. the train( ) method has been executed\n",
    "        \"\"\"\n",
    "        X = X.T\n",
    "        N = X.shape[1]\n",
    "        # Pass the input through the linear transform using the input\n",
    "        Zi = np.dot(self.Ri, X) + np.tile(self.b,(1,N))\n",
    "        \n",
    "        # Obtain the value of Y_i by applying the activation function on Z_i \n",
    "        Yi = # ... Add code here \n",
    "        T_hat = np.dot(self.W_ls, Yi)\n",
    "        return T_hat\n",
    "    \n",
    "    def score(self, X, T):\n",
    "        \"\"\"\n",
    "        This function is similar to the calculate_accuracy function \n",
    "        \"\"\"\n",
    "        T = np.transpose(T)\n",
    "        T_hat = self.predict(X)\n",
    "        Y = np.argmax(T_hat, axis=0)\n",
    "        T = np.argmax(T, axis=0)\n",
    "        \n",
    "        accuracy = np.sum([Y == T]) / Y.shape[0]\n",
    "        return accuracy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b390c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set these parameter dictionaries as you like to search over a grid of parameters\n",
    "param_grid = {'lam': np.logspace(-2, 3, 6).tolist(),  # Parameter grid for lambda\n",
    "              'n': np.linspace(10, 500, 11).astype(np.int16).tolist() # Number of nodes \n",
    "             } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fd6931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ee69bc",
   "metadata": {},
   "source": [
    "### Implementing Grid search and k-fold cross validation.\n",
    "\n",
    "Here, we consider $K=5$ folds as default. \n",
    "\n",
    "- Look up the use of `sklearn.model_selection.GridSearchCV`\n",
    "- Using `sklearn.model_selection.GridSearchCV` tune the hyperparameters `lam, n`. Use options:\n",
    "    - 5 cross validation folds\n",
    "    - refit in case of a new fold\n",
    "    - Choose an appropriate verbosity option to display progress, e.g. 3\n",
    "    - If you have multiple cores on your machine, you can parallelize using `n_jobs`\n",
    "- Display the best hyperparameters through grid search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bfeca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an object of a GridSearchCV class with appropriate options\n",
    "# Add code here\n",
    "grid = # ... # \n",
    "\n",
    "# Get the dataset\n",
    "X_train_all, X_test, T_train_all, T_test = prepare_dataset(dataset_location=\"./Vowel.mat\")\n",
    "\n",
    "# fitting the model for grid search for the data X_train_all, T_train_all using the fit( ) function.\n",
    "# Add code here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2aa9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best parameter after tuning \n",
    "# Add code here (HINT: Check the documentation for GridSearchCV class)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd62d8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_arr = np.unique(grid.cv_results_['param_n'].data)\n",
    "np.array([[m, p['lam']] for p,m in zip(grid.cv_results_['params'],grid.cv_results_['mean_test_score']) if p['n'] == 20])\n",
    "FontSize = 30\n",
    "#csfont = {'fontname':'serif'}\n",
    "plt.rc( 'font', size=30, family=\"Times\" ) \n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "#plt.plot(lam_arr, val_accuracy_lists, 'r-', label=\"Validation Accuracy\")\n",
    "#plt.plot(lam_arr, train_accuracy_lists, 'b-', label=\"Train Accuracy\")\n",
    "for i, n in enumerate(n_arr):\n",
    "    mean_score_lambda_arr = np.array([[m, p['lam']] for p,m in zip(grid.cv_results_['params'],grid.cv_results_['mean_test_score']) if p['n'] == n])\n",
    "    plt.plot(mean_score_lambda_arr[:,1], mean_score_lambda_arr[:,0],  'D-', label=\"Validation Accuracy for n={}\".format(n), linewidth=2)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "plt.grid()\n",
    "plt.xticks(fontsize=FontSize)\n",
    "plt.yticks(fontsize=FontSize)\n",
    "plt.tight_layout()\n",
    "plt.legend(loc='best',fontsize=FontSize)\n",
    "plt.xlabel(\"Hyperparameter lambda\", fontsize=FontSize)\n",
    "plt.ylabel(\"Classification Accuracy\", fontsize=FontSize)\n",
    "plt.title(\"ELM performance on Vowel\", loc='center', fontsize=FontSize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c05a9ca",
   "metadata": {},
   "source": [
    "## Get the accuracy results on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68719926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f1af2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best parameter after tuning \n",
    "print(grid.best_params_) \n",
    "grid_predictions = np.argmax(grid.predict(X_test.T) , axis=0).reshape((-1, 1))\n",
    "# print classification report \n",
    "print(classification_report(np.argmax(T_test, axis=0), grid_predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e23bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_predictions.shape, T_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d618169b",
   "metadata": {},
   "source": [
    "## Using kernels with ELMs\n",
    "\n",
    "### **Training ELM by using linear kernel**\n",
    "\n",
    "We set the regularization hyperparameter alpha, and construct the kernel matrix for training and test data accordingly. In particular, the solution of the kernel method for a pair of $(\\mathbf{y}_j,\\mathbf{t}_j)$ can be written as \n",
    "\\begin{align}\n",
    "    \\hat{\\mathbf{t}}_j = \\mathbf{T} (K + \\alpha \\mathbf{I})^{-1} k(\\mathbf{y}_j),\n",
    "\\end{align}\n",
    "where $\\mathbf{K} \\in \\mathbb{R}^{J \\times J}$ is the kernel matrix with elements $k_{ij}=\\mathbf{k}(\\mathbf{y}_i)^{'}\\mathbf{k}(\\mathbf{y}_j)$ for a given kernel $\\mathbf{k}(\\cdot)$. Note that $k(\\mathbf{y}_j) \\in \\mathbb{R}^{J}$  is the product of sample $\\mathbf{y}_j$ and training set with elements $k_{j}=\\mathbf{k}(\\mathbf{y}_i)^{'}\\mathbf{k}(\\mathbf{y}_j), \\forall i = 1, \\cdots, J$. In matrix form, we can write\n",
    "\\begin{align}\n",
    "    \\hat{\\mathbf{T}}_{\\text{test}} = \\mathbf{T} (K + \\alpha \\mathbf{I})^{-1} K_{\\text{test}},\n",
    "\\end{align}\n",
    "where $K_{\\text{test}} \\in \\mathbb{R}^{J \\times J_{\\text{test}}}$ is the kernel matrix between train and test samples for a given kernel $\\mathbf{k}(\\cdot)$. Now we apply a linear kernel $\\mathbf{k}(\\cdot)$ on the feature vectors of ELM in $\\mathbf{Y}$ to predict the target.\n",
    "\n",
    "- Complete the code to define the linear kernel\n",
    "- Complete the code to define the RBF kernel\n",
    "- Complete the code to compute $\\mathbf{W}_{ker} = \\mathbf{T} (K + \\alpha \\mathbf{I})^{-1}$\n",
    "\n",
    "**NOTE:** We expect the performance of the linear kernel to be roughly similar to what we obtained earlier using the ELM without the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92167884",
   "metadata": {
    "hideCode": false,
    "hideOutput": false
   },
   "outputs": [],
   "source": [
    "class ELM_with_kernel(object):\n",
    "    \n",
    "    def __init__(self, n, alpha, P, kernel_type=\"linear\"):\n",
    "        self.n = n\n",
    "        self.alpha = alpha\n",
    "        self.Ri = self.initialise_random_matrix(M=n, N=P)\n",
    "        self.b = self.initialise_random_matrix(M=n, N=1)\n",
    "        self.W_ker = None\n",
    "        self.kernel_type = kernel_type\n",
    "        \n",
    "    def linear_kernel(self, X, Y):\n",
    "        \"\"\"\n",
    "        Define a linear kernel K(x, y) = x'*y\n",
    "        \"\"\"\n",
    "        # Add code here\n",
    "        # ...\n",
    "        return None\n",
    "        \n",
    "    def rbf_kernel(self, X, Y):\n",
    "        \"\"\"\n",
    "        Defines a RBF kernel K(x, y)\n",
    "        \"\"\"\n",
    "        N1 = X.shape[1]\n",
    "        N2 = Y.shape[1]\n",
    "        n1sq = np.sum(np.square(X),axis=0)\n",
    "        n2sq = np.sum(np.square(Y),axis=0)\n",
    "        D = np.tile(n1sq, (N2, 1)).T + np.tile(n2sq, (N1, 1)) - 2 * np.dot(X.T, Y)  # the element i,j of this matrix are nothing but the individual distance of samples x_i and y_j meaning ||x_i - y_j||_2^2\n",
    "        C = np.tile(n1sq, (N1, 1)).T + np.tile(n1sq, (N1, 1)) - 2 * np.dot(X.T, X)  # the element i,j of this matrix are nothing but the individual distance of samples x_i and x_j meaning ||x_i - x_j||_2^2\n",
    "        temp = np.sum(C)/(N1*N1)    # temp represent the average distance of samples of X from each other. Note that C is zero on the diagonal.\n",
    "        # Add code here to return the value of the RBF kernel using the variables\n",
    "        K = # ...     # note that in this way we don't need to tune the value of temp\n",
    "        return K\n",
    "    \n",
    "    def activation_function(self, Z):\n",
    "        # Here, we basically implement a ReLU( ) activation function\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def initialise_random_matrix(self, M, N):\n",
    "        \"\"\"\n",
    "        Initialize a random matrix that consists of uniformly \n",
    "        distributed values in the range [-1, 1]. Matrix should be \n",
    "        of shape (M, N)\n",
    "        \"\"\"\n",
    "        # Add code here\n",
    "        # ...\n",
    "        return None # Fix this return statement\n",
    "    \n",
    "    def train(self, X, T):\n",
    "        \n",
    "        P = X.shape[0]\n",
    "        N = X.shape[1]\n",
    "        Zi = np.dot(self.Ri, X) + np.tile(self.b,(1,N))\n",
    "        Yi = self.activation_function(Z=Zi)\n",
    "        if self.kernel_type == \"linear\":\n",
    "            self.W_ker = # ... # Add code to compute W_ker using a linear kernel (in float32 format)\n",
    "        elif self.kernel_type == \"gaussian\":\n",
    "            self.W_ker = # ... # Add code to compute W_ker using a RBF kernel (in float32 format)\n",
    "    \n",
    "    def evaluate(self, X_train, X_test):\n",
    "        \n",
    "        N_train = X_train.shape[1]\n",
    "        N_test = X_test.shape[1]\n",
    "        \n",
    "        Zi_train = np.dot(self.Ri, X_train) + np.tile(self.b,(1,N_train))\n",
    "        Yi_train = self.activation_function(Z=Zi_train)\n",
    "        \n",
    "        Zi_test = np.dot(self.Ri, X_test) + np.tile(self.b,(1,N_test))\n",
    "        Yi_test = self.activation_function(Z=Zi_test)\n",
    "        \n",
    "        if self.kernel_type == \"linear\":\n",
    "            K_test = self.linear_kernel(Yi_train, Yi_test)\n",
    "        elif self.kernel_type == \"gaussian\":\n",
    "            K_test = self.rbf_kernel(Yi_train, Yi_test)\n",
    "        \n",
    "        T_hat = np.dot(self.W_ker, K_test)\n",
    "        return T_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118ec35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ELM_with_kernel(X_train, T_train, X_val=None, T_val=None, n=2000, alpha=1e2, kernel_type=\"linear\"):\n",
    "    \n",
    "    elm_net = ELM_with_kernel(n=n, alpha=alpha, P=X_train.shape[0], kernel_type=kernel_type)\n",
    "    elm_net.train(X=X_train, T=T_train)\n",
    "    T_hat_train = elm_net.evaluate(X_train=X_train, X_test=X_train)\n",
    "    T_hat_val = elm_net.evaluate(X_train=X_train, X_test=X_val)\n",
    "    acc_train = calculate_accuracy(T_hat_train, T_train)\n",
    "    acc_val = calculate_accuracy(T_hat_val, T_val)\n",
    "    \n",
    "    return acc_train, acc_val, elm_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83663007",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d271010",
   "metadata": {},
   "source": [
    "### Train an ELM using Linear kernel, using the same parameters optimized earlier for $\\lambda, n$\n",
    "\n",
    "- Complete the code to train the ELM using a Linear kernel\n",
    "- Complete the code to evaluate using the ELM trained using linear kernel on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e373836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train_linear_kernel_star, acc_val_linear_kernel_star, elm_net_linear_kernel_star = # ... # Add code here \n",
    "T_hat_test_linear_kernel = # ... # Add code here\n",
    "acc_test_linear_kernel_star = calculate_accuracy(T_hat_test_linear_kernel, T_test)\n",
    "print(\"Chosen Lambda: {}, Acc_train: {}, Acc_val: {}, Acc_test: {}\".format(grid.best_params_['lam'], \n",
    "                                                                              acc_train_linear_kernel_star, \n",
    "                                                                              acc_val_linear_kernel_star, \n",
    "                                                                              acc_test_linear_kernel_star))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26407b3e",
   "metadata": {},
   "source": [
    "### Training with Gaussian / RBF kernels using the same parameters optimized earlier for $\\lambda, n$\n",
    "\n",
    "- Complete the code to train the ELM using a Gaussian kernel\n",
    "- Complete the code to evaluate using the ELM on the test data\n",
    "\n",
    "**NOTE:** We expect an improvement in the classification accuracy using the RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b124dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train_gaussian_kernel_star, acc_val_gaussian_kernel_star, elm_net_gaussian_kernel_star = # ... # Add code here\n",
    "T_hat_test_gaussian_kernel = # ... # Add code here\n",
    "acc_test_gaussian_kernel_star = calculate_accuracy(T_hat_test_gaussian_kernel, T_test)\n",
    "print(\"Chosen Lambda: {}, Acc_train: {}, Acc_val: {}, Acc_test: {}\".format(grid.best_params_['lam'], \n",
    "                                                                              acc_train_gaussian_kernel_star, \n",
    "                                                                              acc_val_gaussian_kernel_star, \n",
    "                                                                              acc_test_gaussian_kernel_star))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57725613",
   "metadata": {},
   "source": [
    "### Bonus Question: Can you think why we get an improvement using a RBF / gaussian kernel?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bec7b72",
   "metadata": {},
   "source": [
    "Write your answer in text in this markdown cell"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
