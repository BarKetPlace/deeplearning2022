{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed536058",
   "metadata": {},
   "source": [
    "## **Multiclass Classification using ELMs and kernels** \n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "This tutorial demonstrates a multiclass classification using extreme learning machine (ELM) and kernel method. First, you implement ELM by using the least-square method and observe its training and testing accuracy behavior. Second, you compare it with the kernel method when using a linear kernel and investigate the importance of random weights. \n",
    "\n",
    "ELM is implemented using regularized least-square method. Let us use a matrix notation for convenience and denote the training samples by $\\mathbf{X}=[\\mathbf{x}_1, \\cdots, \\mathbf{x}_J] \\in \\mathbb{R}^{P \\times J}$ and their corresponding targets by $\\mathbf{T}=[\\mathbf{t}_1, \\cdots, \\mathbf{t}_J] \\in \\mathbb{R}^{Q \\times J}$. Here, $P$ is the input samples dimension, $Q$ is the target dimension (number of classes), and $J$ is the number of training samples. \n",
    "\n",
    "Training a regularized ELM consists in finding:\n",
    "\\begin{align}\n",
    "    \\mathbf{W}^{\\star} = \\underset{\\mathbf{W}}{\\arg\\min} ||\\mathbf{T} - \\mathbf{W} \\mathbf{Y}||_F^2 + \\lambda ||\\mathbf{W}||_F^2,\n",
    "\\end{align}\n",
    "where $\\mathbf{Y} = \\mathbf{g}(\\mathbf{R}\\mathbf{X}+\\mathbf{B}) \\in \\mathbb{R}^{n \\times J}$ is the output of the activation function $\\mathbf{g}$ and $n$ is the number of hidden neurons. Note that the weight matrix $\\mathbf{R} \\in \\mathbb{R}^{n \\times P}$ and the bias matrix $\\mathbf{B}=[\\mathbf{b},\\cdots,\\mathbf{b}] \\in \\mathbb{R}^{n \\times J}$ are sampled from a Normal distribution and are kept fixed.\n",
    "\n",
    "This minimization problem has the following closed-form solution:\n",
    "\\begin{align}\n",
    "    \\mathbf{W}^{\\star} = \\mathbf{T} \\mathbf{Y}'(\\mathbf{Y}\\mathbf{Y}' + \\lambda \\mathbf{I})^{-1},\n",
    "\\end{align}\n",
    "where $\\mathbf{I} \\in \\mathbb{R}^{n \\times n}$ is the identity matrix. The notation $\\mathbf{Y}'$ denotes the transpose of the matrix $\\mathbf{Y}$. \n",
    "\n",
    "\n",
    "## **Tasks**\n",
    "\n",
    "In this notebook:\n",
    "\n",
    "1. Complete the missing code statements to build an extreme learning machine (ELM) training framework, dependent on parameter $\\lambda$ mentioned above.\n",
    "2. Determine the optimal value of $\\lambda$ using a grid search cross-validation scheme. Plot the validation accuracy of ELM for $\\lambda = 10^{-2}, 10^{-1}, 10^{0}, \\cdots, 10^{3}$ in one figure. Similarly, plot the validation accuracy versus different values of the hyper-parameter $n$.\n",
    "3. What are the best values for the hyperparameters?\n",
    "5. Find the testing accuracy of the linear and Gaussian kernel method when the optimal hyperparameters are used.\n",
    "6. *Bonus question*: Can you provide an explanation of why the ELM with Gaussian kernels perform better than the linear kernel case?\n",
    "\n",
    "\n",
    "Note: the Bonus exercises are not mandatory and will not be used as a grading criterion.\n",
    "\n",
    "\n",
    "\n",
    "## Dataset\n",
    "\n",
    "In this notebook, we use the Vowel dataset which contains 990 English vowel utterances. In the dataset, 15 speakers say 11 vowels 6 times each. The utterances are grouped per speaker and vowel, using integers 0-89. Each utterance is represented by ten floating-point values.\n",
    "The dataset is split into a training and testing set. The training set contains data from groups 0-47 and the test groups 48-89. The goal is to predict the uttered vowel of the utterances in the test set, using information from the training set only. The performance is reported using the number of correct classifications in the test set. For more information regarding the Vowel dataset refer to [this page](https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Vowel+Recognition+-+Deterding+Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd464b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from scipy.io import loadmat\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd5ea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the versions of the libraries in use\n",
    "print(\"numpy=={}\".format(np.__version__))\n",
    "print(\"scikit-learn=={}\".format(sklearn.__version__))\n",
    "print(\"matplotlib=={}\".format(matplotlib.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cf9cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dd8595",
   "metadata": {},
   "source": [
    "# **Definition of the required functions and class**\n",
    "Here, we define some of the functions that we are going to use later in this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606d38d8",
   "metadata": {},
   "source": [
    "## Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8a692e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to load the dataset\n",
    "def prepare_dataset(dataset_location=\"./Vowel.mat\"):\n",
    "    \"\"\"\n",
    "    An example of how the dataset looks in practice:\n",
    "    +-------+--------+-------+---------+\n",
    "        | vowel |  word  | vowel |  word   | \n",
    "        +-------+--------+-------+---------+\n",
    "        |  i    |  heed  |  O    |  hod    |\n",
    "        |  I    |  hid   |  C:   |  hoard  |\n",
    "        |  E    |  head  |  U    |  hood   |\n",
    "        |  A    |  had   |  u:   |  who'd  |\n",
    "        |  a:   |  hard  |  3:   |  heard  |\n",
    "        |  Y    |  hud   |       |         |\n",
    "        +-------+--------+-------+---------+\n",
    "    \"\"\"\n",
    "    X = loadmat(dataset_location)[\"featureMat\"]\n",
    "    Y = loadmat(dataset_location)[\"labelMat\"]\n",
    "    X_train, X_test = X[:, :528].astype(np.float32), X[:, 528:].astype(np.float32)\n",
    "    Y_train, Y_test = Y[:, :528].astype(np.float32), Y[:, 528:].astype(np.float32) \n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bed192",
   "metadata": {},
   "source": [
    "## Define a function to calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba32ace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(S, T):\n",
    "    # S: predictions\n",
    "    # T: given targets (ground truth)\n",
    "    Y = np.argmax(S, axis=0)\n",
    "    T = np.argmax(T, axis=0)\n",
    "    accuracy = np.sum([Y == T]) / Y.shape[0] # This computes a classification accuracy\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73924100",
   "metadata": {},
   "source": [
    "## Define a class for an Extreme learning machine network\n",
    "\n",
    "- Complete the following class definition for defining an extreme learning machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74a58ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELM(object):\n",
    "    \"\"\"\n",
    "    This defines a class for the extreme learning machine (ELM)\n",
    "    \"\"\"\n",
    "    def __init__(self, n, lam, P):\n",
    "        self.n = n # Number of hidden nodes \n",
    "        self.lam = lam # Initialize the hyperparameter lambda, which is to be tuned\n",
    "        self.Ri = self.initialise_random_matrix(M=n, N=P) # Initialize the random matrix \n",
    "        self.b = self.initialise_random_matrix(M=n, N=1) # Initialize the bias, also randomly\n",
    "        self.W_ls = None # Initially set to None, because we compute it using least squares\n",
    "    \n",
    "    def activation_function(self, Z):\n",
    "        # Here, we basically implement a ReLU( ) activation function\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def initialise_random_matrix(self, M, N):\n",
    "        \"\"\"\n",
    "        Initialize a random matrix that consists of uniformly \n",
    "        distributed values in the range [-1, 1]. Matrix should be \n",
    "        of shape (M, N)\n",
    "        \"\"\"\n",
    "        # Add code here\n",
    "        # ...\n",
    "        return None # Fix this return statement\n",
    "\n",
    "\n",
    "    def LS(self, Y, T):\n",
    "        \"\"\"Solve the optimization problem as regularized least-squares\"\"\"\n",
    "        P = Y.shape[0] # Dimensionality of the input\n",
    "        # Implement the equation T*Y'*inv(Y*Y' + lambda*I) as the least squares solution\n",
    "        W_ls = # ... Add code here\n",
    "        return W_ls\n",
    "    \n",
    "    def train(self, X, T):\n",
    "        \"\"\"\n",
    "        Train the ELM network. Note that here training indicates simply calculating\n",
    "        the matrix W_ls. Since, we have a closed form expression, this amounts to only\n",
    "        calling the relevant method that we have wrote earlier. \n",
    "        \"\"\"\n",
    "        N = X.shape[1] # Number of samples in the data X\n",
    "        \n",
    "        # Pass the input through the linear transform using the input\n",
    "        Zi = np.dot(self.Ri, X) + np.tile(self.b,(1,N))\n",
    "        \n",
    "        # Obtain the value of Y_i by applying the activation function on Z_i \n",
    "        Yi = # ... Add code here \n",
    "        \n",
    "        # Obtain the least squares matrix W_ls\n",
    "        self.W_ls = self.LS(Y=Yi, T=T)\n",
    "    \n",
    "    def evaluate(self, X):\n",
    "        \"\"\"\n",
    "        This function provides predictions using the ELM network, provided that\n",
    "        the network has been already trained, i.e. the train( ) method has been executed\n",
    "        \"\"\"\n",
    "        N = X.shape[1] # Number of samples in the data X\n",
    "        \n",
    "        # Pass the input through the linear transform using the input\n",
    "        Zi = np.dot(self.Ri, X) + np.tile(self.b,(1,N))\n",
    "        \n",
    "        # Obtain the value of Y_i by applying the activation function on Z_i \n",
    "        Yi = self.activation_function(Z=Zi)\n",
    "        \n",
    "        # Get the prediction using the W_ls\n",
    "        T_hat = # ... # Add code here \n",
    "        return T_hat\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e327e8",
   "metadata": {},
   "source": [
    "## Define the training pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1313ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ELM(X_train, T_train, X_val=None, T_val=None, n=2000, lam=1e2):\n",
    "    \"\"\"\n",
    "    This function trains and evaluates the ELM by instantiating the ELM class,\n",
    "    train the network and then evaluate both the training and the validation\n",
    "    data\n",
    "    \"\"\"\n",
    "    elm_net = ELM(n=n, lam=lam, P=X_train.shape[0])\n",
    "    elm_net.train(X=X_train, T=T_train)\n",
    "    T_hat_train = elm_net.evaluate(X=X_train)\n",
    "    T_hat_val = elm_net.evaluate(X=X_val)\n",
    "    acc_train = calculate_accuracy(T_hat_train, T_train)\n",
    "    acc_val = calculate_accuracy(T_hat_val, T_val)\n",
    "    \n",
    "    return acc_train, acc_val, elm_net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a355a063",
   "metadata": {},
   "source": [
    "# **Training ELM by using least-square method**\n",
    "\n",
    "We set the number of hidden neurons $n$ and the regularization hyperparameter $\\lambda$, and construct a single layer neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fffb1ed",
   "metadata": {},
   "source": [
    "### Get an initial estimate of accuracy from an ELM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63da73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset\n",
    "X_train, X_test, T_train, T_test = prepare_dataset(dataset_location=\"./Vowel.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888add63",
   "metadata": {},
   "outputs": [],
   "source": [
    "P= X_train.shape[0] # Input features\n",
    "Q = T_train.shape[0] # Output label size\n",
    "N_train = X_train.shape[1] # Number of samples in training data\n",
    "N_test = X_test.shape[1] # Number of samples in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3961b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(P, Q, N_train, N_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5925f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tr_val_data(X, T, train_val_split=0.8):\n",
    "    \"\"\"\n",
    "    Splits the data randomly into training and validation data in the ratio 80:20\n",
    "    \"\"\"\n",
    "    N = X.shape[1]\n",
    "    indices = np.random.permutation(N)\n",
    "    num_train_samples = int(train_val_split * len(indices))\n",
    "    num_val_samples = len(indices) - num_train_samples\n",
    "    train_indices = indices[:num_train_samples]\n",
    "    val_indices = indices[num_train_samples:]\n",
    "    X_train, T_train = X[:, train_indices], T[:, train_indices]\n",
    "    X_val, T_val = X[:, val_indices], T[:, val_indices]\n",
    "    print(X_train.shape, T_train.shape, X_val.shape, T_val.shape)\n",
    "    return X_train, T_train, X_val, T_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1e9377",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_to_val_split = 0.8\n",
    "X_train, T_train, X_val, T_val = split_tr_val_data(X=X_train, T=T_train, train_val_split=tr_to_val_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f457b3",
   "metadata": {},
   "source": [
    "### Execute this cell to train the ELM using an arbitrary $\\lambda$. \n",
    "\n",
    "We note that executing this cell several times can lead to different results. On average the classification accuracy is poor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7b7b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train_star, acc_val_star, elm_net_star = train_ELM(X_train, T_train, X_val, T_val, n=200, lam=1e-14)\n",
    "T_hat_test = elm_net_star.evaluate(X=X_test)\n",
    "acc_test_star = calculate_accuracy(T_hat_test, T_test)\n",
    "print(\"Lambda: {}, Acc_train: {}, Acc_val: {}, Acc_test: {}\".format(1e-2, \n",
    "                                                                  acc_train_star, \n",
    "                                                                  acc_val_star, \n",
    "                                                                  acc_test_star))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1c54db",
   "metadata": {},
   "source": [
    "## Grid search CV: Sweeping over a set of parameters $\\lambda$ to find the ELM with most suited architecture\n",
    "Here we want to use existing libraries for [grid search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
    "This requires our ELM class to implement certain methods.\n",
    "\n",
    "- We reimplement an `ELM_estimator` by subclassing from `BaseEstimator`\n",
    "- We add standard methods to the train and evaluate the model: `fit()`, `predict()` and `score()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9460552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class ELM_estimator(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, n=2000, lam=1e-5, P=10):\n",
    "        super().__init__()\n",
    "        self.n = n # Number of hidden nodes \n",
    "        self.P = P # Input dimensionality\n",
    "        self.lam = lam # Initialize the hyperparameter lambda, which is to be tuned\n",
    "        self.Ri = self.initialise_random_matrix(M=self.n, N=self.P) # Initialize the random matrix \n",
    "        self.b = self.initialise_random_matrix(M=n, N=1) # Initialize the bias, also randomly\n",
    "        self.W_ls = None # Initially set to None, because we compute it using\n",
    "        \n",
    "    def activation_function(self, Z):\n",
    "         # Here, we basically implement a ReLU( ) activation function\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def initialise_random_matrix(self, M, N):\n",
    "        \"\"\"\n",
    "        Initialize a random matrix that consists of uniformly \n",
    "        distributed values in the range [-1, 1]. Matrix should be \n",
    "        of shape (M, N)\n",
    "        \"\"\"\n",
    "        # Add code here\n",
    "        # ...\n",
    "        return None # Fix this return statement\n",
    "\n",
    "    def LS(self, Y, T):\n",
    "        \"\"\"Solve the optimization problem as regularized least-squares\"\"\"\n",
    "        P = Y.shape[0] # Dimensionality of the input\n",
    "        # Implement the equation T*Y'*inv(Y*Y' + lambda*I) as the least squares solution\n",
    "        W_ls = # ... Add code here\n",
    "        return W_ls\n",
    "    \n",
    "    def fit(self, X, T):\n",
    "        \"\"\"\n",
    "        Train the ELM network. Note that here training indicates simply calculating\n",
    "        the matrix W_ls. Since, we have a closed form expression, this amounts to only\n",
    "        calling the relevant method that we have wrote earlier. \n",
    "        \"\"\"\n",
    "        X, T = X.T, T.T\n",
    "        N = X.shape[1]\n",
    "        \n",
    "        # Pass the input through the linear transform using the input\n",
    "        Zi = np.dot(self.Ri, X) + np.tile(self.b,(1,N))\n",
    "        \n",
    "        # Obtain the value of Y_i by applying the activation function on Z_i \n",
    "        Yi = # ... Add code here \n",
    "        \n",
    "        self.W_ls = self.LS(Y=Yi, T=T)\n",
    "        return self\n",
    "        #T_hat = np.dot(self.W_ls, Yi)\n",
    "        #return T_hat\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        This function provides predictions using the ELM network, provided that\n",
    "        the network has been already trained, i.e. the train( ) method has been executed\n",
    "        \"\"\"\n",
    "        X = X.T\n",
    "        N = X.shape[1]\n",
    "        # Pass the input through the linear transform using the input\n",
    "        Zi = np.dot(self.Ri, X) + np.tile(self.b,(1,N))\n",
    "        \n",
    "        # Obtain the value of Y_i by applying the activation function on Z_i \n",
    "        Yi = # ... Add code here \n",
    "        T_hat = np.dot(self.W_ls, Yi)\n",
    "        return T_hat\n",
    "    \n",
    "    def score(self, X, T):\n",
    "        \"\"\"\n",
    "        This function is similar to the calculate_accuracy function \n",
    "        \"\"\"\n",
    "        T = np.transpose(T)\n",
    "        T_hat = self.predict(X)\n",
    "        Y = np.argmax(T_hat, axis=0)\n",
    "        T = np.argmax(T, axis=0)\n",
    "        \n",
    "        accuracy = np.sum([Y == T]) / Y.shape[0]\n",
    "        return accuracy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b390c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parameters are specified in a dictionnaries. The keys will be passed to our class __init__ method.\n",
    "# You can set the support and granularity of the parameters using np.logspaces or np.linspaces\n",
    "\n",
    "param_grid = {'lam': None,# ... Add code here    # Parameter grid for lambda\n",
    "              'n': None # ... Add code here      # Number of nodes \n",
    "             } "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22929714",
   "metadata": {},
   "source": [
    "### Using Grid search and k-fold cross validation.\n",
    "\n",
    "Here, we consider $K=5$ folds as default. \n",
    "\n",
    "- Look up the use of `sklearn.model_selection.GridSearchCV` in the  [doc](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
    "\n",
    "- Using `sklearn.model_selection.GridSearchCV` find the best hyperparameters `lam, n`. Use options:\n",
    "    - 5 cross validation folds\n",
    "    - refit in case of a new fold\n",
    "    - verbosity=3 (to display progress)\n",
    "    - If you have multiple cores on your machine, you can parallelize using `n_jobs`\n",
    "- Display the best hyperparameters through grid search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fd6931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bfeca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an object of a GridSearchCV class with appropriate options\n",
    "# Add code here\n",
    "grid = # ... # \n",
    "\n",
    "# Get the dataset\n",
    "X_train_all, X_test, T_train_all, T_test = prepare_dataset(dataset_location=\"./Vowel.mat\")\n",
    "\n",
    "# fitting the model for grid search for the data X_train_all, T_train_all using the fit( ) function.\n",
    "# Add code here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2aa9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best parameter after tuning \n",
    "# Add code here (HINT: Check the documentation for GridSearchCV class)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd62d8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_arr = np.unique(grid.cv_results_['param_n'].data)\n",
    "np.array([[m, p['lam']] for p,m in zip(grid.cv_results_['params'],grid.cv_results_['mean_test_score']) if p['n'] == 20])\n",
    "FontSize = 30\n",
    "#csfont = {'fontname':'serif'}\n",
    "plt.rc( 'font', size=30, family=\"Times\" ) \n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "for i, n in enumerate(n_arr):\n",
    "    mean_score_lambda_arr = np.array([[m, p['lam']] for p,m in zip(grid.cv_results_['params'],grid.cv_results_['mean_test_score']) if p['n'] == n])\n",
    "    plt.plot(mean_score_lambda_arr[:,1], mean_score_lambda_arr[:,0],  'D-', label=\"Validation Accuracy for n={}\".format(n), linewidth=2)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "plt.grid()\n",
    "plt.xticks(fontsize=FontSize)\n",
    "plt.yticks(fontsize=FontSize)\n",
    "plt.tight_layout()\n",
    "plt.legend(loc='best',fontsize=FontSize)\n",
    "plt.xlabel(\"Hyperparameter lambda\", fontsize=FontSize)\n",
    "plt.ylabel(\"Classification Accuracy\", fontsize=FontSize)\n",
    "plt.title(\"ELM performance on Vowel\", loc='center', fontsize=FontSize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c05a9ca",
   "metadata": {},
   "source": [
    "## Get the accuracy results on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68719926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f1af2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best parameter after tuning \n",
    "print(grid.best_params_) \n",
    "grid_predictions = np.argmax(grid.predict(X_test.T) , axis=0).reshape((-1, 1))\n",
    "# print classification report \n",
    "print(classification_report(np.argmax(T_test, axis=0), grid_predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e23bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_predictions.shape, T_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d618169b",
   "metadata": {},
   "source": [
    "## Using kernels with ELMs\n",
    "\n",
    "### **Training ELM by using linear kernel**\n",
    "\n",
    "We set the regularization hyperparameter alpha, and construct the kernel matrix for training and test data accordingly. In particular, the solution of the kernel method for a pair of $(\\mathbf{y}_j,\\mathbf{t}_j)$ can be written as \n",
    "\\begin{align}\n",
    "    \\hat{\\mathbf{t}}_j = \\mathbf{T} (K + \\alpha \\mathbf{I})^{-1} k(\\mathbf{y}_j),\n",
    "\\end{align}\n",
    "where $\\mathbf{K} \\in \\mathbb{R}^{J \\times J}$ is the kernel matrix with elements $k_{ij}=\\mathbf{k}(\\mathbf{y}_i)^{'}\\mathbf{k}(\\mathbf{y}_j)$ for a given kernel $\\mathbf{k}(\\cdot)$. Note that $k(\\mathbf{y}_j) \\in \\mathbb{R}^{J}$  is the product of sample $\\mathbf{y}_j$ and training set with elements $k_{j}=\\mathbf{k}(\\mathbf{y}_i)^{'}\\mathbf{k}(\\mathbf{y}_j), \\forall i = 1, \\cdots, J$. In matrix form, we can write\n",
    "\\begin{align}\n",
    "    \\hat{\\mathbf{T}}_{\\text{test}} = \\mathbf{T} (K + \\alpha \\mathbf{I})^{-1} K_{\\text{test}},\n",
    "\\end{align}\n",
    "where $K_{\\text{test}} \\in \\mathbb{R}^{J \\times J_{\\text{test}}}$ is the kernel matrix between train and test samples for a given kernel $\\mathbf{k}(\\cdot)$. Now we apply a linear kernel $\\mathbf{k}(\\cdot)$ on the feature vectors of ELM in $\\mathbf{Y}$ to predict the target.\n",
    "\n",
    "Complete the code and\n",
    "- Define the linear kernel $K(\\mathbf{x}, \\mathbf{y}) = \\mathbf{x}^{\\top}\\mathbf{y}$\n",
    "- Define the RBF kernel $K(\\mathbf{x}, \\mathbf{y}) = \\exp\\left(-\\gamma\\Vert\\mathbf{x} - \\mathbf{y}\\Vert^2_2\\right)$\n",
    "- Compute $\\mathbf{W}_{ker} = \\mathbf{T} (K + \\alpha \\mathbf{I})^{-1}$\n",
    "\n",
    "**NOTE:** We expect the performance of the linear kernel to be similar to what we obtained above using the LS-ELM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92167884",
   "metadata": {
    "hideCode": false,
    "hideOutput": false
   },
   "outputs": [],
   "source": [
    "class ELM_with_kernel(object):\n",
    "    \n",
    "    def __init__(self, n, alpha, P, kernel_type=\"linear\", gamma=1.0):\n",
    "        self.n = n\n",
    "        self.alpha = alpha\n",
    "        self.Ri = self.initialise_random_matrix(M=n, N=P)\n",
    "        self.b = self.initialise_random_matrix(M=n, N=1)\n",
    "        self.W_ker = None\n",
    "        self.kernel_type = kernel_type\n",
    "        self.gamma = gamma # Hyperparameter for RBF kernel\n",
    "        \n",
    "    def linear_kernel(self, X, Y):\n",
    "        \"\"\"\n",
    "        Define a linear kernel K(x, y) = x'*y\n",
    "        \"\"\"\n",
    "        # Add code here\n",
    "        # ...\n",
    "        return None\n",
    "        \n",
    "    def rbf_kernel(self, X, Y):\n",
    "        \"\"\"\n",
    "        Defines a RBF kernel K(x, y)\n",
    "        \"\"\"\n",
    "        N1 = X.shape[1]\n",
    "        N2 = Y.shape[1]\n",
    "        n1sq = np.sum(np.square(X),axis=0)\n",
    "        n2sq = np.sum(np.square(Y),axis=0)\n",
    "        # This computes the matrix ||x_i - y_j||_2^2 in a vectorized manner\n",
    "        D = np.tile(n1sq, (N2, 1)).T + np.tile(n2sq, (N1, 1)) - 2 * np.dot(X.T, Y) \n",
    "\n",
    "        # Add code here to return the value of the RBF kernel using the variables and hyperparameters\n",
    "        K = # ...     # note that in this way we don't need to tune the value of temp\n",
    "        return K\n",
    "    \n",
    "    def activation_function(self, Z):\n",
    "        # Here, we basically implement a ReLU( ) activation function\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def initialise_random_matrix(self, M, N):\n",
    "        \"\"\"\n",
    "        Initialize a random matrix that consists of uniformly \n",
    "        distributed values in the range [-1, 1]. Matrix should be \n",
    "        of shape (M, N)\n",
    "        \"\"\"\n",
    "        # Add code here\n",
    "        # ...\n",
    "        return None # Fix this return statement\n",
    "    \n",
    "    def train(self, X, T):\n",
    "        \n",
    "        P = X.shape[0]\n",
    "        N = X.shape[1]\n",
    "        Zi = np.dot(self.Ri, X) + np.tile(self.b,(1,N))\n",
    "        Yi = self.activation_function(Z=Zi)\n",
    "        if self.kernel_type == \"linear\":\n",
    "            self.W_ker = # ... # Add code to compute W_ker using a linear kernel (in float32 format)\n",
    "        elif self.kernel_type == \"gaussian\":\n",
    "            self.W_ker = # ... # Add code to compute W_ker using a RBF kernel (in float32 format)\n",
    "    \n",
    "    def evaluate(self, X_train, X_test):\n",
    "        \n",
    "        N_train = X_train.shape[1]\n",
    "        N_test = X_test.shape[1]\n",
    "        \n",
    "        Zi_train = np.dot(self.Ri, X_train) + np.tile(self.b,(1,N_train))\n",
    "        Yi_train = self.activation_function(Z=Zi_train)\n",
    "        \n",
    "        Zi_test = np.dot(self.Ri, X_test) + np.tile(self.b,(1,N_test))\n",
    "        Yi_test = self.activation_function(Z=Zi_test)\n",
    "        \n",
    "        if self.kernel_type == \"linear\":\n",
    "            K_test = self.linear_kernel(Yi_train, Yi_test)\n",
    "        elif self.kernel_type == \"gaussian\":\n",
    "            K_test = self.rbf_kernel(Yi_train, Yi_test)\n",
    "        \n",
    "        T_hat = np.dot(self.W_ker, K_test)\n",
    "        return T_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118ec35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ELM_with_kernel(X_train, T_train, X_val=None, T_val=None, n=2000, alpha=1e2, kernel_type=\"linear\"):\n",
    "    \n",
    "    elm_net = ELM_with_kernel(n=n, alpha=alpha, P=X_train.shape[0], kernel_type=kernel_type)\n",
    "    elm_net.train(X=X_train, T=T_train)\n",
    "    T_hat_train = elm_net.evaluate(X_train=X_train, X_test=X_train)\n",
    "    T_hat_val = elm_net.evaluate(X_train=X_train, X_test=X_val)\n",
    "    acc_train = calculate_accuracy(T_hat_train, T_train)\n",
    "    acc_val = calculate_accuracy(T_hat_val, T_val)\n",
    "    \n",
    "    return acc_train, acc_val, elm_net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d271010",
   "metadata": {},
   "source": [
    "### Train an ELM using Linear kernel, using the same parameters optimized earlier for $\\lambda, n$\n",
    "\n",
    "- Complete the code to train the ELM using a Linear kernel\n",
    "- Complete the code to evaluate using the ELM trained using linear kernel on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d93a681",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hyper-parameters:\")\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e373836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train_linear_kernel_star, acc_val_linear_kernel_star, elm_net_linear_kernel_star = # ... # Add code here \n",
    "T_hat_test_linear_kernel = # ... # Add code here\n",
    "acc_test_linear_kernel_star = calculate_accuracy(T_hat_test_linear_kernel, T_test)\n",
    "print(\"Linear kernel, Acc_train: {}, Acc_val: {}, Acc_test: {}\".format(acc_train_linear_kernel_star, \n",
    "                                                                      acc_val_linear_kernel_star, \n",
    "                                                                      acc_test_linear_kernel_star))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26407b3e",
   "metadata": {},
   "source": [
    "### Training with Gaussian / RBF kernels using the same parameters optimized earlier for $\\lambda, n$\n",
    "\n",
    "- Complete the code to train the ELM using a Gaussian kernel\n",
    "- Complete the code to evaluate using the ELM on the test data\n",
    "\n",
    "**NOTE:** We expect an improvement in the classification accuracy using the RBF kernel. You can also check for further improvement by tuning the $\\gamma$ parameter of the RBF kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b124dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train_gaussian_kernel_star, acc_val_gaussian_kernel_star, elm_net_gaussian_kernel_star = # ... # Add code here\n",
    "T_hat_test_gaussian_kernel = # ... # Add code here\n",
    "acc_test_gaussian_kernel_star = calculate_accuracy(T_hat_test_gaussian_kernel, T_test)\n",
    "print(\"RBF kernel, Acc_train: {}, Acc_val: {}, Acc_test: {}\".format(acc_train_gaussian_kernel_star, \n",
    "                                                                    acc_val_gaussian_kernel_star, \n",
    "                                                                    acc_test_gaussian_kernel_star))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57725613",
   "metadata": {},
   "source": [
    "### Bonus Question: In your opinion, why do we get an improvement using a RBF / gaussian kernel?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bec7b72",
   "metadata": {},
   "source": [
    "Write your answer in text in this markdown cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681f9dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
