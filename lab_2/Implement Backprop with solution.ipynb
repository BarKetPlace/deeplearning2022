{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of backpropagation\n",
    "\n",
    "Estimated time: 1h\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook we train a 1 hidden layer neural network on the artificial two moons dataset.\n",
    "The particularity is that we manually implement the update rule of a backpropagation algorithm.\n",
    "The goal is to give an insight on how machine learning toolboxes work under the hood, and to understand why we need these toolboxes to build complex training pipelines.\n",
    "Here we do not use such toolboxes, but rather implement a back propagation algorithm using the numpy library.\n",
    "\n",
    "The code for this tutorial was found here:\n",
    "- Walkthrough backpropagation algorithm: https://cs231n.github.io/optimization-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e63ba5dc1467>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we use is the standard two-moons dataset, which is a non-linear binary classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = datasets.make_moons(n_samples=1000, noise=0.05)\n",
    "y = np.hstack([1-y[:,None],y[:,None]])\n",
    "\n",
    "colors = 'rb'\n",
    "c_ = [colors[yy] for yy in y[:,0]]\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x[:,0], x[:,1], c=c_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first initialize the hyper-parameters of our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network definitions\n",
    "\n",
    "d = 2 # Input space dimension\n",
    "n1 = 2 # First layer hidden size\n",
    "q = 2  # Target space dimension\n",
    "\n",
    "# Initialize NN\n",
    "hidden_sizes = [(d,n1), (n1,q)]\n",
    "print(hidden_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before computing the backpropagation algorithm, we must define the forward pass algorithm of our network.\n",
    "\n",
    "- Task 1: Write the forward pass for the network pre-defined above, assuming that the non-linearity is the relu function\n",
    "\n",
    "The loss we propose to use is the mean squared error function.\n",
    "\n",
    "- Task 2: Write the gradient of the loss function wrt to the output of the network you have computed in the previous task.\n",
    "\n",
    "The core of the back propagation algorithm is implemented for you, all that's left is to implement the weight update rule.\n",
    "\n",
    "- Task 3: Implement the weight update rule with a learning rate of 1\n",
    "\n",
    "If every thing works as expected, you should see the loss converging.\n",
    "\n",
    "Now that we have a functional back propagation algorithm, we can try changing the learning rate and see how this impacts the convergence speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight initialization\n",
    "\n",
    "w1 = np.random.randn(hidden_sizes[0][1],hidden_sizes[0][0])\n",
    "w2 = np.random.randn(hidden_sizes[1][1],hidden_sizes[1][0])\n",
    "\n",
    "# Training loop\n",
    "nepochs = 50\n",
    "\n",
    "loss = np.zeros(nepochs)\n",
    "\n",
    "for ix in range(nepochs):\n",
    "    n_samples = x.shape[0]\n",
    "    # Forward pass: compute y_pred    \n",
    "    #\n",
    "    ## Task 1\n",
    "    a1 = x\n",
    "    z2 = a1 @ w1\n",
    "    a2 = np.maximum(z2, 0)\n",
    "    z3 = a2 @ w2\n",
    "    # a3 = np.maximum(z3, 0)\n",
    "    y_pred = z3\n",
    "    \n",
    "    # Compute loss\n",
    "    loss[ix] = (0.5) * np.square(y_pred - y).mean()\n",
    "\n",
    "    # Gradient of loss wrt network's output\n",
    "    # \n",
    "    ## Task 2\n",
    "    d3 = y_pred - y\n",
    "    \n",
    "    grad2 = a2.T @ d3 / n_samples\n",
    "    d2_tmp = d3 @ w2.T\n",
    "    d2 = d2_tmp.copy()\n",
    "    d2[z2 <= 0] = 0 #  d2 = d2 * derivative of ReLU function\n",
    "    grad1 = a1.T @ d2 / n_samples\n",
    "    \n",
    "    # Update weights\n",
    "    #\n",
    "    ## Task 3\n",
    "    w1 = w1 - grad1\n",
    "    w2 = w2 - grad2\n",
    "\n",
    "\n",
    "# Plot the loss\n",
    "plt.figure()\n",
    "plt.plot(loss,'k',lw=3)\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "## Task 1\n",
    "a1 = x\n",
    "z2 = a1 @ w1\n",
    "a2 = np.maximum(z2, 0)\n",
    "z3 = a2 @ w2\n",
    "a3 = np.maximum(z3, 0)\n",
    "\n",
    "## Task 2\n",
    "d3 = y_pred - y\n",
    "\n",
    "## Task 3\n",
    "w1 = w1 - grad1\n",
    "w2 = w2 - grad2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning2022",
   "language": "python",
   "name": "deeplearning2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
